<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>看论文---第一弹 | yanzu_blog</title><meta name="author" content="yanzu"><meta name="copyright" content="yanzu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning—通过深度强化学习实现目标驱动的自主探索摘要  本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局">
<meta property="og:type" content="article">
<meta property="og:title" content="看论文---第一弹">
<meta property="og:url" content="http://yanzu.tech/2025/05/26/17/index.html">
<meta property="og:site_name" content="yanzu_blog">
<meta property="og:description" content="Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning—通过深度强化学习实现目标驱动的自主探索摘要  本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://picbed.yanzu.tech/img/post_cover/p17.png">
<meta property="article:published_time" content="2025-05-26T04:07:22.695Z">
<meta property="article:modified_time" content="2025-05-26T12:08:00.000Z">
<meta property="article:author" content="yanzu">
<meta property="article:tag" content="Learning">
<meta property="article:tag" content="ROS1">
<meta property="article:tag" content="RL">
<meta property="article:tag" content="paper-reading">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://picbed.yanzu.tech/img/post_cover/p17.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yanzu.tech/2025/05/26/17/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const hour = new Date().getHours()
          const isNight = hour <= 6 || hour >= 18
          if (theme === undefined) isNight ? activateDarkMode() : activateLightMode()
          else theme === 'light' ? activateLightMode() : activateDarkMode()
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"已切换为繁体中文","cht_to_chs":"已切换为简体中文","day_to_night":"已切换为深色模式","night_to_day":"已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '看论文---第一弹',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  isShuoshuo: false
}</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/nav.css"><link rel="stylesheet" href="/css/neon_light.css"><link rel="stylesheet" href="/css/universe.css"><link rel="stylesheet" href="/css/info_card.css"><span id="fps"></span><style type="text/css"> .aplayer.aplayer-fixed { left: 20px !important; /* 靠左侧 20px */ top: 50% !important; /* 居中对齐 */ transform: translateY(-50%) !important; /* 让播放器真正居中 */ width: 300px !important; /* 播放器宽度 */ background: transparent !important; /* 透明背景 */ box-shadow: none !important; /* 去掉阴影 */ border: none !important; /* 去掉边框 */ } .aplayer.aplayer-fixed .aplayer-body { background: transparent !important; /* 去掉内部背景 */ } </style><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (true) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image: url(/images/bg.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post type-RL" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(http://picbed.yanzu.tech/img/post_cover/p17.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/images/nav_logo.png" alt="Logo"></a><a class="nav-page-title" href="/"><span class="site-name">看论文---第一弹</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">看论文---第一弹</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-26T04:07:22.695Z" title="发表于 2025-05-26 12:07:22">2025-05-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-26T12:08:00.000Z" title="更新于 2025-05-26 20:08:00">2025-05-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Goal-Driven-Autonomous-Exploration-Through-Deep-Reinforcement-Learning—通过深度强化学习实现目标驱动的自主探索"><a href="#Goal-Driven-Autonomous-Exploration-Through-Deep-Reinforcement-Learning—通过深度强化学习实现目标驱动的自主探索" class="headerlink" title="Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning—通过深度强化学习实现目标驱动的自主探索"></a>Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning—通过深度强化学习实现目标驱动的自主探索</h1><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><blockquote>
<blockquote>
<h4 id="本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。"><a href="#本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。" class="headerlink" title="本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。"></a>本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。</h4></blockquote>
<h4 id="自主导航系统"><a href="#自主导航系统" class="headerlink" title="自主导航系统"></a>自主导航系统</h4><h4 id="用于通过DRL对未知环境进行目标驱动的探索"><a href="#用于通过DRL对未知环境进行目标驱动的探索" class="headerlink" title="用于通过DRL对未知环境进行目标驱动的探索"></a>用于通过DRL对未知环境进行目标驱动的探索</h4><h4 id="获取可能导航方向的兴趣点-POI"><a href="#获取可能导航方向的兴趣点-POI" class="headerlink" title="获取可能导航方向的兴趣点(POI)"></a>获取可能导航方向的兴趣点(POI)</h4><h4 id="根据可用数据选择最佳航路点"><a href="#根据可用数据选择最佳航路点" class="headerlink" title="根据可用数据选择最佳航路点"></a>根据可用数据选择最佳航路点</h4><h4 id="缓解反应式导航中的局部最优问题"><a href="#缓解反应式导航中的局部最优问题" class="headerlink" title="缓解反应式导航中的局部最优问题"></a>缓解反应式导航中的局部最优问题</h4><h4 id="采用的是TD3算法（双延迟深度确定性策略梯度）"><a href="#采用的是TD3算法（双延迟深度确定性策略梯度）" class="headerlink" title="采用的是TD3算法（双延迟深度确定性策略梯度）"></a>采用的是TD3算法（双延迟深度确定性策略梯度）</h4><blockquote>
<h4 id="它是在DDPG算法的基础上进行的扩展"><a href="#它是在DDPG算法的基础上进行的扩展" class="headerlink" title="它是在DDPG算法的基础上进行的扩展"></a>它是在DDPG算法的基础上进行的扩展</h4><h4 id="DDPG（Deep-Deterministic-Policy-Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略"><a href="#DDPG（Deep-Deterministic-Policy-Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略" class="headerlink" title="DDPG（Deep Deterministic Policy Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略"></a>DDPG（Deep Deterministic Policy Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略</h4><h4 id="详细介绍：https-spinningup-openai-com-en-latest-algorithms-ddpg-html-background"><a href="#详细介绍：https-spinningup-openai-com-en-latest-algorithms-ddpg-html-background" class="headerlink" title="详细介绍：https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background"></a>详细介绍：<a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background">https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background</a></h4><h4 id="DDPG-对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的-Q-函数会严重高估-Q-值，进而导致策略崩溃，因为策略会利用-Q-函数中的误差"><a href="#DDPG-对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的-Q-函数会严重高估-Q-值，进而导致策略崩溃，因为策略会利用-Q-函数中的误差" class="headerlink" title="DDPG 对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的 Q 函数会严重高估 Q 值，进而导致策略崩溃，因为策略会利用 Q 函数中的误差"></a>DDPG 对超参数和其他调优手段<strong>非常敏感</strong>，表现常常不稳定，它的一个常见失败模式是：所学习的 Q 函数会<strong>严重高估 Q 值</strong>，进而导致策略崩溃，因为策略会利用 Q 函数中的误差</h4><h4 id="而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题："><a href="#而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：" class="headerlink" title="而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题："></a>而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：</h4><blockquote>
<ol>
<li><h4 id="截断的双Q学习：TD3-同时学习两个-Q-函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险"><a href="#截断的双Q学习：TD3-同时学习两个-Q-函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险" class="headerlink" title="截断的双Q学习：TD3 同时学习两个 Q 函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险"></a>截断的双Q学习：TD3 同时学习两个 Q 函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险</h4></li>
<li><h4 id="延迟的策略更新：TD3-中，策略网络（Actor）和目标网络的更新频率低于-Q-函数的更新频率，建议是，更新两次-Q-网络，只更新-一次策略网络，以提高训练稳定性"><a href="#延迟的策略更新：TD3-中，策略网络（Actor）和目标网络的更新频率低于-Q-函数的更新频率，建议是，更新两次-Q-网络，只更新-一次策略网络，以提高训练稳定性" class="headerlink" title="延迟的策略更新：TD3 中，策略网络（Actor）和目标网络的更新频率低于 Q 函数的更新频率，建议是，更新两次 Q 网络，只更新 一次策略网络，以提高训练稳定性"></a>延迟的策略更新：TD3 中，策略网络（Actor）和目标网络的更新频率<strong>低于 Q 函数</strong>的更新频率，建议是，更新两次 Q 网络，只更新 <strong>一次策略网络</strong>，以提高训练稳定性</h4></li>
<li><h4 id="目标策略平滑：TD3-在计算目标-Q-值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的-Q-值更平滑，从而防止策略去“钻空子”利用-Q-函数中的误差"><a href="#目标策略平滑：TD3-在计算目标-Q-值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的-Q-值更平滑，从而防止策略去“钻空子”利用-Q-函数中的误差" class="headerlink" title="目标策略平滑：TD3 在计算目标 Q 值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的 Q 值更平滑，从而防止策略去“钻空子”利用 Q 函数中的误差"></a>目标策略平滑：TD3 在计算目标 Q 值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的 Q 值更平滑，从而防止策略去“钻空子”利用 Q 函数中的误差</h4></li>
</ol>
</blockquote>
<h4 id="详细介绍：https-spinningup-openai-com-en-latest-algorithms-td3-html"><a href="#详细介绍：https-spinningup-openai-com-en-latest-algorithms-td3-html" class="headerlink" title="详细介绍：https://spinningup.openai.com/en/latest/algorithms/td3.html"></a>详细介绍：<a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/algorithms/td3.html">https://spinningup.openai.com/en/latest/algorithms/td3.html</a></h4></blockquote>
</blockquote>
<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><blockquote>
<h4 id="完全自主的目标驱动探索是一个包含两个方面的问题"><a href="#完全自主的目标驱动探索是一个包含两个方面的问题" class="headerlink" title="完全自主的目标驱动探索是一个包含两个方面的问题"></a><strong>完全自主的目标驱动探索</strong>是一个包含两个方面的问题</h4><blockquote>
<h4 id="首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。"><a href="#首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。" class="headerlink" title="首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。"></a>首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。</h4><h4 id="其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。"><a href="#其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。" class="headerlink" title="其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。"></a>其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。</h4><h4 id="这就引入了DRL"><a href="#这就引入了DRL" class="headerlink" title="这就引入了DRL"></a>这就引入了DRL</h4><h4 id="但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。"><a href="#但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。" class="headerlink" title="但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。"></a>但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。</h4><blockquote>
<h4 id="DRL的反应性特征"><a href="#DRL的反应性特征" class="headerlink" title="DRL的反应性特征"></a>DRL的反应性特征</h4><blockquote>
<h4 id="DRL中的大多策略都是基于当前状态做出决策的，也即-reactive-policy（反应式策略），也就是具有马尔可夫性"><a href="#DRL中的大多策略都是基于当前状态做出决策的，也即-reactive-policy（反应式策略），也就是具有马尔可夫性" class="headerlink" title="DRL中的大多策略都是基于当前状态做出决策的，也即 reactive policy（反应式策略），也就是具有马尔可夫性"></a>DRL中的大多策略都是基于当前状态做出决策的，也即 reactive policy（反应式策略），也就是具有马尔可夫性</h4><h4 id="这会导致："><a href="#这会导致：" class="headerlink" title="这会导致："></a>这会导致：</h4><ul>
<li><h4 id="agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作"><a href="#agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作" class="headerlink" title="agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作"></a>agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作</h4></li>
<li><h4 id="没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力"><a href="#没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力" class="headerlink" title="没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力"></a>没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力</h4></li>
<li><h4 id="动作的选择往往基于短期奖励最大化，而非长期全局最优"><a href="#动作的选择往往基于短期奖励最大化，而非长期全局最优" class="headerlink" title="动作的选择往往基于短期奖励最大化，而非长期全局最优"></a>动作的选择往往基于短期奖励最大化，而非长期全局最优</h4></li>
</ul>
<h4 id="这种策略适用于快速反应、实时避障等场景"><a href="#这种策略适用于快速反应、实时避障等场景" class="headerlink" title="这种策略适用于快速反应、实时避障等场景"></a>这种策略适用于快速反应、实时避障等场景</h4></blockquote>
<h4 id="全局信息的缺乏"><a href="#全局信息的缺乏" class="headerlink" title="全局信息的缺乏"></a>全局信息的缺乏</h4><blockquote>
<h4 id="大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。"><a href="#大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。" class="headerlink" title="大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。"></a>大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。</h4></blockquote>
<h4 id="局部最优问题"><a href="#局部最优问题" class="headerlink" title="局部最优问题"></a>局部最优问题</h4><blockquote>
<h4 id="系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。"><a href="#系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。" class="headerlink" title="系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。"></a>系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。</h4></blockquote>
</blockquote>
</blockquote>
<h4 id="本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。"><a href="#本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。" class="headerlink" title="本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。"></a>本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。</h4><h4 id="该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。"><a href="#该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。" class="headerlink" title="该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。"></a>该系统从机器人周围的局部环境中提取<strong>兴趣点（POI）</strong>，对其进行评估，并从中选取一个作为航路点（waypoint）。</h4><h4 id="这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。"><a href="#这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。" class="headerlink" title="这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。"></a>这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。</h4><h4 id="机器人依据该策略进行运动，无需对周围环境进行完整建图。"><a href="#机器人依据该策略进行运动，无需对周围环境进行完整建图。" class="headerlink" title="机器人依据该策略进行运动，无需对周围环境进行完整建图。"></a>机器人依据该策略进行运动，无需对周围环境进行完整建图。</h4><h4 id="POI"><a href="#POI" class="headerlink" title="POI"></a>POI</h4><blockquote>
<h4 id="这里所说的-POI-兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI-可以作为导航路径的候选点，也即中间航路点waypoint。"><a href="#这里所说的-POI-兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI-可以作为导航路径的候选点，也即中间航路点waypoint。" class="headerlink" title="这里所说的 POI 兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI 可以作为导航路径的候选点，也即中间航路点waypoint。"></a>这里所说的 POI 兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI 可以作为导航路径的候选点，也即中间航路点waypoint。</h4><h4 id="POI的确定"><a href="#POI的确定" class="headerlink" title="POI的确定"></a>POI的确定</h4><blockquote>
<h4 id="激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个-POI。"><a href="#激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个-POI。" class="headerlink" title="激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个 POI。"></a>激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个 POI。</h4><h4 id="非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。"><a href="#非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。" class="headerlink" title="非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。"></a>非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。</h4></blockquote>
</blockquote>
<h4 id="导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。"><a href="#导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。" class="headerlink" title="导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。"></a>导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。</h4><p><img src="http://picbed.yanzu.tech/img/paper_read/1/1.jpg"></p>
</blockquote>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><blockquote>
<h4 id="导航结构分两部分："><a href="#导航结构分两部分：" class="headerlink" title="导航结构分两部分："></a>导航结构分两部分：</h4><blockquote>
<h4 id="具有最优航路点选择机制的全局导航与建图模块"><a href="#具有最优航路点选择机制的全局导航与建图模块" class="headerlink" title="具有最优航路点选择机制的全局导航与建图模块"></a>具有<strong>最优航路点选择机制</strong>的<strong>全局导航与建图</strong>模块</h4><h4 id="基于深度强化学习的局部导航模块"><a href="#基于深度强化学习的局部导航模块" class="headerlink" title="基于深度强化学习的局部导航模块"></a>基于<strong>深度强化学习</strong>的<strong>局部导航</strong>模块</h4></blockquote>
<h4 id="系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。"><a href="#系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。" class="headerlink" title="系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。"></a>系统首先从环境中提取<strong>兴趣点（POI）</strong>，并依据设定的评估标准选择一个<strong>最优航路点</strong>。</h4><h4 id="在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。"><a href="#在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。" class="headerlink" title="在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。"></a>在每一个导航步骤中，系统会将该航路点以<strong>相对于机器人当前位置与朝向的极坐标形式</strong>输入神经网络。</h4><h4 id="随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。"><a href="#随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。" class="headerlink" title="随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。"></a>随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。</h4><h4 id="在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。"><a href="#在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。" class="headerlink" title="在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。"></a>在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的<strong>地图构建（建图）</strong>。</h4><h3 id="A-全局导航"><a href="#A-全局导航" class="headerlink" title="A.全局导航"></a>A.全局导航</h3><blockquote>
<h4 id="为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。"><a href="#为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。" class="headerlink" title="为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。"></a>为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择<strong>用于局部导航的中间航路点</strong>。</h4><h4 id="机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。"><a href="#机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。" class="headerlink" title="机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。"></a>机器人不仅需要被引导前往目标，还必须在行进过程中<strong>探索周围环境</strong>，以便在遇到死路时能够识别出可能的替代路径。</h4><h4 id="鉴于没有预先提供的环境信息，所有可能的-POI-必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。"><a href="#鉴于没有预先提供的环境信息，所有可能的-POI-必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。" class="headerlink" title="鉴于没有预先提供的环境信息，所有可能的 POI 必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。"></a>鉴于没有预先提供的环境信息，所有可能的 POI 必须从<strong>机器人当前的周边环境中提取</strong>，并<strong>存储在内存中</strong>以供后续使用。</h4><h4 id="如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。"><a href="#如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。" class="headerlink" title="如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。"></a>如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。</h4><h4 id="在机器人已经访问过的位置，激光雷达不会再提取新的-POI。"><a href="#在机器人已经访问过的位置，激光雷达不会再提取新的-POI。" class="headerlink" title="在机器人已经访问过的位置，激光雷达不会再提取新的 POI。"></a>在机器人已经访问过的位置，激光雷达不会再提取新的 POI。</h4><h4 id="此外，如果某个-POI-被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。"><a href="#此外，如果某个-POI-被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。" class="headerlink" title="此外，如果某个 POI 被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。"></a>此外，如果某个 POI 被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。</h4><h4 id="获取新的POI的方法：就是上面提到的POI的确定"><a href="#获取新的POI的方法：就是上面提到的POI的确定" class="headerlink" title="获取新的POI的方法：就是上面提到的POI的确定"></a>获取新的POI的方法：就是上面提到的POI的确定</h4><h4 id="从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。"><a href="#从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。" class="headerlink" title="从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。"></a>从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。</h4><h4 id="a-蓝色-POI-是通过激光雷达测距数据之间的间隙提取的；"><a href="#a-蓝色-POI-是通过激光雷达测距数据之间的间隙提取的；" class="headerlink" title="(a) 蓝色 POI 是通过激光雷达测距数据之间的间隙提取的；"></a>(a) 蓝色 POI 是通过激光雷达测距数据之间的间隙提取的；</h4><h4 id="b-蓝色-POI-是从非数值型的激光读数中提取的。"><a href="#b-蓝色-POI-是从非数值型的激光读数中提取的。" class="headerlink" title="(b)蓝色 POI 是从非数值型的激光读数中提取的。"></a>(b)蓝色 POI 是从非数值型的激光读数中提取的。</h4><p><img src="http://picbed.yanzu.tech/img/paper_read/1/2.jpg"></p>
<h4 id="在时刻-t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based-Distance-Limited-Exploration，简称-IDLE）来选择最优的航路点。IDLE-方法通过以下方式评估每个候选-POI-的适应度："><a href="#在时刻-t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based-Distance-Limited-Exploration，简称-IDLE）来选择最优的航路点。IDLE-方法通过以下方式评估每个候选-POI-的适应度：" class="headerlink" title="在时刻 t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based Distance Limited Exploration，简称 IDLE）来选择最优的航路点。IDLE 方法通过以下方式评估每个候选 POI 的适应度："></a>在时刻 <em>t</em>，从当前可用的兴趣点（POI）中，使用<strong>基于信息的距离受限探索方法</strong>（Information-based Distance Limited Exploration，简称 <strong>IDLE</strong>）来选择最优的航路点。IDLE 方法通过以下方式评估每个候选 POI 的适应度：</h4><p>$$<br>h(c_i) &#x3D; \tanh\left(\frac{e^{\left(\frac{d(p_t, c_i)}{l_2 - l_1}\right)^2}}{e^{\left(\frac{l_2}{l_2 - l_1}\right)^2}}\right) l_2 + d(c_i, g) + e^{I_{i,t}}<br>\tag{1}<br>$$</p>
<h4 id="每个候选兴趣点-c-索引为-i-的得分-h-由三部分组成"><a href="#每个候选兴趣点-c-索引为-i-的得分-h-由三部分组成" class="headerlink" title="每个候选兴趣点 c (索引为 i) 的得分 h 由三部分组成"></a>每个候选兴趣点 c (索引为 i) 的得分 h 由三部分组成</h4><h4 id="其中，机器人在时刻-t-的位置-p-t-与候选兴趣点-c-i-之间的欧几里得距离分量-d-p-t-c-i-被表示为一个双曲正切函数（tanh）形式："><a href="#其中，机器人在时刻-t-的位置-p-t-与候选兴趣点-c-i-之间的欧几里得距离分量-d-p-t-c-i-被表示为一个双曲正切函数（tanh）形式：" class="headerlink" title="其中，机器人在时刻 t 的位置 p_t 与候选兴趣点 c_i 之间的欧几里得距离分量 d(p_t, c_i) 被表示为一个双曲正切函数（tanh）形式："></a>其中，机器人在时刻 t 的位置 p_t 与候选兴趣点 <em>c_i</em> 之间的欧几里得距离分量 d(p_t, c_i) 被表示为一个双曲正切函数（tanh）形式：</h4><p>$$<br>\tanh\left(\frac{e^{\left(\frac{d(p_t, c_i)}{l_2 - l_1}\right)^2}}{e^{\left(\frac{l_2}{l_2 - l_1}\right)^2}}\right) l_2<br>\tag{2}<br>$$</p>
<h4 id="其中，e-是自然对数的底（欧拉数），l-1-和-l-2-是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。-这部分是一个距离惩罚项"><a href="#其中，e-是自然对数的底（欧拉数），l-1-和-l-2-是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。-这部分是一个距离惩罚项" class="headerlink" title="其中，e 是自然对数的底（欧拉数），l_1 和 l_2 是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。 这部分是一个距离惩罚项"></a>其中，<em>e</em> 是自然对数的底（欧拉数），<em>l_1</em> 和 <em>l_2</em> 是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。 这部分是一个距离惩罚项</h4><h4 id="注："><a href="#注：" class="headerlink" title="注："></a>注：</h4><blockquote>
<h4 id="分子中指数部分的-d-p-t-c-i-l-2-l-1-这是在对距离-d-做归一化处理，把距离尺度转化为无单位的比值，l-2-l-1-是距离窗口，也就是期望-d-落在这个区间里，这样做的目的是让距离-d-的大小相对于“允许的距离范围”-（l-2-l-1）-来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。"><a href="#分子中指数部分的-d-p-t-c-i-l-2-l-1-这是在对距离-d-做归一化处理，把距离尺度转化为无单位的比值，l-2-l-1-是距离窗口，也就是期望-d-落在这个区间里，这样做的目的是让距离-d-的大小相对于“允许的距离范围”-（l-2-l-1）-来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。" class="headerlink" title="分子中指数部分的 d(p_t,c_i)&#x2F;(l_2 - l_1) 这是在对距离 d 做归一化处理，把距离尺度转化为无单位的比值，l_2 - l_1 是距离窗口，也就是期望 d 落在这个区间里，这样做的目的是让距离 d 的大小相对于“允许的距离范围” （l_2 - l_1） 来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。"></a>分子中指数部分的 d(p_t,c_i)&#x2F;(l_2 - l_1) 这是在对距离 d 做归一化处理，把距离尺度转化为无单位的比值，l_2 - l_1 是距离窗口，也就是期望 d 落在这个区间里，这样做的目的是让距离 d 的大小相对于“允许的距离范围” （l_2 - l_1） 来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。</h4><h4 id="l-2-l-2-l-1-是对-l-2-进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e-x-e-y-这个分式的归一化处理。"><a href="#l-2-l-2-l-1-是对-l-2-进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e-x-e-y-这个分式的归一化处理。" class="headerlink" title="l_2 &#x2F; (l_2 - l_1) 是对 l_2 进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e^x &#x2F; e^y 这个分式的归一化处理。"></a>l_2 &#x2F; (l_2 - l_1) 是对 l_2 进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e^x &#x2F; e^y 这个分式的归一化处理。</h4><h4 id="通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。"><a href="#通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。" class="headerlink" title="通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。"></a>通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。</h4><h4 id="通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。"><a href="#通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。" class="headerlink" title="通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。"></a>通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的<strong>平滑衰减</strong>效果。</h4><h4 id="这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。"><a href="#这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。" class="headerlink" title="这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。"></a>这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。</h4><h4 id="使用双曲正切函数-tanh-将指数结果映射到-0-1-之间，起到非线性缩放和平滑归一化的作用。"><a href="#使用双曲正切函数-tanh-将指数结果映射到-0-1-之间，起到非线性缩放和平滑归一化的作用。" class="headerlink" title="使用双曲正切函数 tanh() 将指数结果映射到 (0, 1) 之间，起到非线性缩放和平滑归一化的作用。"></a>使用双曲正切函数 tanh() 将指数结果映射到 (0, 1) 之间，起到非线性缩放和平滑归一化的作用。</h4><h4 id="最后乘以-l-2-对距离分量进行尺度调整，映射到-0-l-2-之间"><a href="#最后乘以-l-2-对距离分量进行尺度调整，映射到-0-l-2-之间" class="headerlink" title="最后乘以 l_2 对距离分量进行尺度调整，映射到 (0, l_2)之间"></a>最后乘以 l_2 对距离分量进行尺度调整，映射到 (0, l_2)之间</h4><h4 id="这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于-l-1-l-2-之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定"><a href="#这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于-l-1-l-2-之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定" class="headerlink" title="这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于 l_1~l_2 之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定"></a>这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于 l_1~l_2 之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定</h4></blockquote>
<h4 id="第二个分量-d-c-i-g-表示候选兴趣点-c-i-与全局目标点-g-之间的欧几里得距离（全局目标距离）。"><a href="#第二个分量-d-c-i-g-表示候选兴趣点-c-i-与全局目标点-g-之间的欧几里得距离（全局目标距离）。" class="headerlink" title="第二个分量 d(c_i, g) 表示候选兴趣点 c_i 与全局目标点 g 之间的欧几里得距离（全局目标距离）。"></a>第二个分量 d(c_i, g) 表示候选兴趣点 c_i 与全局目标点 g 之间的欧几里得距离（全局目标距离）。</h4><h4 id="最后，时刻-t-的地图信息得分（信息增益激励项）表示为："><a href="#最后，时刻-t-的地图信息得分（信息增益激励项）表示为：" class="headerlink" title="最后，时刻 t 的地图信息得分（信息增益激励项）表示为："></a>最后，时刻 t 的地图信息得分（信息增益激励项）表示为：</h4><p>$$<br>e^{I_{i,t}}<br>\tag{3}<br>$$</p>
<h4 id="其中，I-i-t-的计算方式如下："><a href="#其中，I-i-t-的计算方式如下：" class="headerlink" title="其中，I_{i,t} 的计算方式如下："></a>其中，<em>I_{i,t}</em> 的计算方式如下：</h4><p>$$<br>I_{i,t} &#x3D; \frac{\sum\limits_{w&#x3D;-\frac{k}{2}}^{\frac{k}{2}} \sum\limits_{h&#x3D;-\frac{k}{2}}^{\frac{k}{2}} C(x+w)(y+h)}{k^2}<br>\tag{4}<br>$$</p>
<h4 id="其中，k-表示用于计算候选点周围信息的卷积核大小，候选点的坐标为-x-和-y-，而-w-和-h-分别表示卷积核的宽度和高度。"><a href="#其中，k-表示用于计算候选点周围信息的卷积核大小，候选点的坐标为-x-和-y-，而-w-和-h-分别表示卷积核的宽度和高度。" class="headerlink" title="其中，k 表示用于计算候选点周围信息的卷积核大小，候选点的坐标为 x 和 y ，而 w 和 h 分别表示卷积核的宽度和高度。"></a>其中，k 表示用于计算候选点周围信息的卷积核大小，候选点的坐标为 x 和 y ，而 w 和 h 分别表示卷积核的宽度和高度。</h4><h4 id="在公式-1-中，具有最小-IDLE-得分的兴趣点（POI）被选为用于局部导航的最优航路点。"><a href="#在公式-1-中，具有最小-IDLE-得分的兴趣点（POI）被选为用于局部导航的最优航路点。" class="headerlink" title="在公式 (1) 中，具有最小 IDLE 得分的兴趣点（POI）被选为用于局部导航的最优航路点。"></a>在公式 (1) 中，具有最小 IDLE 得分的兴趣点（POI）被选为用于局部导航的最优航路点。</h4><h4 id="注：-1"><a href="#注：-1" class="headerlink" title="注："></a>注：</h4><blockquote>
<h4 id="I-i-t-表示候选兴趣点（POI）c-i-在时间-t-的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此-POI-附近还有未探索、未知或值得探索的区域。"><a href="#I-i-t-表示候选兴趣点（POI）c-i-在时间-t-的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此-POI-附近还有未探索、未知或值得探索的区域。" class="headerlink" title="I_{i,t} 表示候选兴趣点（POI）c_i 在时间 t 的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此 POI 附近还有未探索、未知或值得探索的区域。"></a>I_{i,t} 表示候选兴趣点（POI）c_i 在时间 t 的<strong>信息得分</strong>，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此 POI 附近还有<strong>未探索、未知或值得探索的区域</strong>。</h4><h4 id="C-x-y-是地图上-x-y-点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。"><a href="#C-x-y-是地图上-x-y-点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。" class="headerlink" title="C(x, y) 是地图上 (x, y) 点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。"></a>C(x, y) 是地图上 (x, y) 点的<strong>置信值或不确定度值</strong>，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。</h4><h4 id="双重求和：它是在点-x-y-的周围取了一个大小为-k-k-的滑动窗口，然后对该窗口内所有位置的-C-值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。"><a href="#双重求和：它是在点-x-y-的周围取了一个大小为-k-k-的滑动窗口，然后对该窗口内所有位置的-C-值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。" class="headerlink" title="双重求和：它是在点 (x, y) 的周围取了一个大小为 k * k 的滑动窗口，然后对该窗口内所有位置的 C 值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。"></a>双重求和：它是在点 (x, y) 的周围取了一个大小为 k * k 的滑动窗口，然后对该窗口内所有位置的 C 值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。</h4><h4 id="除以-k-2-是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在-0-1-内，便于指数函数处理。"><a href="#除以-k-2-是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在-0-1-内，便于指数函数处理。" class="headerlink" title="除以 k^2 是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在 [0, 1]内，便于指数函数处理。"></a>除以 k^2 是做了一个<strong>均值操作</strong>，即把窗口内的信息总量归一化为平均信息值，得到的值落在 [0, 1]内，便于指数函数处理。</h4><h4 id="其目的就是鼓励探索未知区域"><a href="#其目的就是鼓励探索未知区域" class="headerlink" title="其目的就是鼓励探索未知区域"></a>其目的就是鼓励探索未知区域</h4></blockquote>
<h4 id="注意到，h-的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个-l-2-的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的"><a href="#注意到，h-的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个-l-2-的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的" class="headerlink" title="注意到，h 的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个 *l_2 的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的"></a>注意到，h 的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个 *l_2 的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的</h4></blockquote>
<h3 id="B-局部导航"><a href="#B-局部导航" class="headerlink" title="B.局部导航"></a>B.局部导航</h3><blockquote>
<h4 id="使用基于-双延迟深度确定性策略梯度-TD3，一种-Actor-Critic-架构-的神经网络架构来训练运动策略"><a href="#使用基于-双延迟深度确定性策略梯度-TD3，一种-Actor-Critic-架构-的神经网络架构来训练运动策略" class="headerlink" title="使用基于 双延迟深度确定性策略梯度(TD3，一种 Actor-Critic 架构)的神经网络架构来训练运动策略"></a>使用基于 双延迟深度确定性策略梯度(TD3，一种 Actor-Critic 架构)的神经网络架构来训练运动策略</h4><h4 id="局部环境信息和目标航点相对于-agent-位置的极坐标一起，作为状态输入-s-传入-TD3-的Actor-网络中"><a href="#局部环境信息和目标航点相对于-agent-位置的极坐标一起，作为状态输入-s-传入-TD3-的Actor-网络中" class="headerlink" title="局部环境信息和目标航点相对于 agent 位置的极坐标一起，作为状态输入 s 传入 TD3 的Actor 网络中"></a>局部环境信息和目标航点相对于 agent 位置的极坐标一起，作为状态输入 s 传入 TD3 的Actor 网络中</h4><h4 id="该-Actor-网络由两个全连接（FC）层组成，每一层后面都接有-ReLU（修正线性单元）激活函数。"><a href="#该-Actor-网络由两个全连接（FC）层组成，每一层后面都接有-ReLU（修正线性单元）激活函数。" class="headerlink" title="该 Actor 网络由两个全连接（FC）层组成，每一层后面都接有 ReLU（修正线性单元）激活函数。"></a>该 Actor 网络由两个<strong>全连接（FC）层</strong>组成，每一层后面都接有 <strong>ReLU（修正线性单元）激活函数</strong>。</h4><h4 id="最后一层与输出层相连，输出两个动作参数-a，分别表示机器人的线速度-a-1-和角速度-a-2"><a href="#最后一层与输出层相连，输出两个动作参数-a，分别表示机器人的线速度-a-1-和角速度-a-2" class="headerlink" title="最后一层与输出层相连，输出两个动作参数 a，分别表示机器人的线速度 a_1 和角速度 a_2"></a>最后一层与输出层相连，输出两个动作参数 a，分别表示机器人的线速度 a_1 和角速度 a_2</h4><h4 id="输出层采用-tanh-激活函数，将输出限制在区间-−1-1-内"><a href="#输出层采用-tanh-激活函数，将输出限制在区间-−1-1-内" class="headerlink" title="输出层采用 tanh 激活函数，将输出限制在区间 (−1,1) 内"></a>输出层采用 <strong>tanh 激活函数</strong>，将输出限制在区间 (−1,1) 内</h4><h4 id="在将动作应用于环境之前，它们会按以下方式缩放为实际速度值："><a href="#在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：" class="headerlink" title="在将动作应用于环境之前，它们会按以下方式缩放为实际速度值："></a>在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：</h4><p>$$<br>a &#x3D; \left[ v_{\max} \left( \frac{a_1 + 1}{2} \right), \omega_{\max} a_2 \right],<br>\tag{5}<br>$$</p>
<h4 id="最大线速度-v-max，最大角速度-ω-max"><a href="#最大线速度-v-max，最大角速度-ω-max" class="headerlink" title="最大线速度 v_max，最大角速度 ω_max"></a>最大线速度 v_max，最大角速度 ω_max</h4><h4 id="由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。"><a href="#由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。" class="headerlink" title="由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。"></a>由于激光雷达只记录机器人前方的数据，因此<strong>不考虑向后的运动</strong>，并将<strong>线速度调整为仅为正值</strong>。</h4><h4 id="状态-动作对的-Q-值-Q-s-a-由两个-Critic-网络进行评估。这两个-Critic-网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。"><a href="#状态-动作对的-Q-值-Q-s-a-由两个-Critic-网络进行评估。这两个-Critic-网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。" class="headerlink" title="状态-动作对的 Q 值 Q(s,a) 由两个 Critic 网络进行评估。这两个 Critic 网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。"></a>状态-动作对的 Q 值 Q(s,a) 由两个 <strong>Critic 网络</strong>进行评估。这两个 Critic 网络具有相同的结构，但其参数更新是<strong>延迟进行</strong>的，从而允许它们在参数上产生差异（避免完全同步）。</h4><h4 id="Critic-网络以状态-s-和动作-a-的组合作为输入"><a href="#Critic-网络以状态-s-和动作-a-的组合作为输入" class="headerlink" title="Critic 网络以状态 s 和动作 a 的组合作为输入"></a>Critic 网络以状态 s 和动作 a 的组合作为输入</h4><h4 id="其中，状态-s-首先被送入一个全连接层，并接上一个-ReLU-激活函数，输出为-L-s"><a href="#其中，状态-s-首先被送入一个全连接层，并接上一个-ReLU-激活函数，输出为-L-s" class="headerlink" title="其中，状态 s 首先被送入一个全连接层，并接上一个 ReLU 激活函数，输出为 L_s"></a>其中，状态 s 首先被送入一个<strong>全连接层</strong>，并接上一个 ReLU 激活函数，输出为 L_s</h4><h4 id="该层的输出-L-s-以及动作-a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为-τ1-和-τ2，随后，这两个结果按如下方式进行组合："><a href="#该层的输出-L-s-以及动作-a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为-τ1-和-τ2，随后，这两个结果按如下方式进行组合：" class="headerlink" title="该层的输出 L_s 以及动作 a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为 τ1 和 τ2，随后，这两个结果按如下方式进行组合："></a>该层的输出 L_s 以及动作 a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为 τ1 和 τ2，随后，这两个结果按如下方式进行组合：</h4><p>$$<br>L_c &#x3D; L_sW_{\tau_1} + aW_{\tau_2} + b_{\tau_2},<br>\tag{6}<br>$$</p>
<h4 id="其中，L-c-是组合全连接层（CFC）的输出，W-τ-1-和-W-τ-2-分别是-τ1-和-τ2-的权重，b-τ-2-是-τ-2-的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的-Q-值。最终，从两个-Critic-网络中选择较小的-Q-值，作为最后的-Critic-输出，以此来限制对状态-动作值的过高估计。"><a href="#其中，L-c-是组合全连接层（CFC）的输出，W-τ-1-和-W-τ-2-分别是-τ1-和-τ2-的权重，b-τ-2-是-τ-2-的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的-Q-值。最终，从两个-Critic-网络中选择较小的-Q-值，作为最后的-Critic-输出，以此来限制对状态-动作值的过高估计。" class="headerlink" title="其中，L_c 是组合全连接层（CFC）的输出，W_τ_1 和 W_τ_2 分别是 τ1 和 τ2 的权重，b_τ_2 是 τ_2 的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的 Q 值。最终，从两个 Critic 网络中选择较小的 Q 值，作为最后的 Critic 输出，以此来限制对状态-动作值的过高估计。"></a>其中，L_c 是组合全连接层（CFC）的输出，W_τ_1 和 W_τ_2 分别是 τ1 和 τ2 的权重，b_τ_2 是 τ_2 的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含<strong>一个参数</strong>，表示对应状态-动作对的 <strong>Q 值</strong>。最终，从两个 Critic 网络中<strong>选择较小的 Q 值</strong>，作为最后的 Critic 输出，以此来<strong>限制对状态-动作值的过高估计</strong>。</h4><h4 id="完整的网络架构如图"><a href="#完整的网络架构如图" class="headerlink" title="完整的网络架构如图"></a>完整的网络架构如图</h4><p><img src="http://picbed.yanzu.tech/img/paper_read/1/3.jpg"></p>
<h4 id="TD3-网络结构包括-actor-和-critic-两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC-层指的是变换全连接层-τ，CFC-层指的是组合全连接层-Lc。"><a href="#TD3-网络结构包括-actor-和-critic-两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC-层指的是变换全连接层-τ，CFC-层指的是组合全连接层-Lc。" class="headerlink" title="TD3 网络结构包括 actor 和 critic 两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC 层指的是变换全连接层 τ，CFC 层指的是组合全连接层 Lc。"></a>TD3 网络结构包括 actor 和 critic 两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC 层指的是变换全连接层 τ，CFC 层指的是组合全连接层 Lc。</h4><h4 id="策略的奖励依据以下函数进行评估-奖励函数"><a href="#策略的奖励依据以下函数进行评估-奖励函数" class="headerlink" title="策略的奖励依据以下函数进行评估(奖励函数)"></a>策略的奖励依据以下函数进行评估(奖励函数)</h4><p>$$<br>r(s_t, a_t) &#x3D;<br>\begin{cases}<br>r_g &amp; \text{if } D_t &lt; \eta D \<br>r_c &amp; \text{if collision} \<br>v - |\omega| &amp; \text{otherwise},<br>\end{cases},<br>\tag{7}<br>$$</p>
<h4 id="在时间步-t时，状态-动作对-s-t-a-t-的奖励-r-取决于以下三种情况："><a href="#在时间步-t时，状态-动作对-s-t-a-t-的奖励-r-取决于以下三种情况：" class="headerlink" title="在时间步 t时，状态-动作对 (s_t, a_t) 的奖励 r 取决于以下三种情况："></a>在时间步 t时，状态-动作对 (s_t, a_t) 的奖励 r 取决于以下三种情况：</h4><ul>
<li><h4 id="如果当前时间步与目标点的距离-D-t-小于阈值-η-D，则给予一个正的目标奖励-r-g（也就是鼓励）"><a href="#如果当前时间步与目标点的距离-D-t-小于阈值-η-D，则给予一个正的目标奖励-r-g（也就是鼓励）" class="headerlink" title="如果当前时间步与目标点的距离 D_t 小于阈值 η_D，则给予一个正的目标奖励 r_g（也就是鼓励）"></a>如果当前时间步与目标点的距离 D_t 小于阈值 η_D，则给予一个正的目标奖励 r_g（也就是鼓励）</h4></li>
<li><h4 id="如果检测到碰撞，则给予一个负的碰撞惩罚-r-c（惩罚）"><a href="#如果检测到碰撞，则给予一个负的碰撞惩罚-r-c（惩罚）" class="headerlink" title="如果检测到碰撞，则给予一个负的碰撞惩罚 r_c（惩罚）"></a>如果检测到碰撞，则给予一个负的碰撞惩罚 r_c（惩罚）</h4></li>
<li><h4 id="如果以上两种情况均未发生，则根据当前的线速度-v-和角速度-ω-给予即时奖励。"><a href="#如果以上两种情况均未发生，则根据当前的线速度-v-和角速度-ω-给予即时奖励。" class="headerlink" title="如果以上两种情况均未发生，则根据当前的线速度 v 和角速度 ω 给予即时奖励。"></a>如果以上两种情况均未发生，则根据当前的线速度 v 和角速度 ω 给予即时奖励。</h4></li>
</ul>
<h4 id="为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下："><a href="#为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：" class="headerlink" title="为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下："></a>为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：</h4><p>$$<br>r_{t-i} &#x3D; r(s_{t-i}, a_{t-i}) + \frac{r_g}{i}, \quad \forall i \in {1, 2, 3, \ldots, n},<br>\tag{8}<br>$$</p>
<h4 id="其中，n-表示前-n-个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的-n-个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。"><a href="#其中，n-表示前-n-个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的-n-个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。" class="headerlink" title="其中，n 表示前 n 个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的 n 个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。"></a>其中，n 表示前 n 个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的 n 个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。</h4></blockquote>
<h3 id="C-探索与建图"><a href="#C-探索与建图" class="headerlink" title="C.探索与建图"></a>C.探索与建图</h3><blockquote>
<h4 id="机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。"><a href="#机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。" class="headerlink" title="机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。"></a>机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。</h4></blockquote>
</blockquote>
<h3 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h3><blockquote>
<h4 id="算法过程："><a href="#算法过程：" class="headerlink" title="算法过程："></a>算法过程：</h4><blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># 输入参数</span><br><span class="line">global_goal		# 全局目标点，即最终导航的目标位置</span><br><span class="line">δ			# 导航至全局目标的距离阈值,用于判断机器人是否“足够接近”目标点</span><br><span class="line"></span><br><span class="line"># 主循环 直到达到全局目标</span><br><span class="line">while(reached_global_goal != True)&#123;	# 判断是否已达到全局目标</span><br><span class="line"></span><br><span class="line">	read sensor data # 读取传感器数据：如激光雷达、相机、里程计等</span><br><span class="line">	</span><br><span class="line">	update map from sensor data # 根据传感器数据更新地图：构建或完善占据栅格地图</span><br><span class="line">	</span><br><span class="line">	Obtain new POI # 获取新的兴趣点 POI,可能是探索边界或未知区域的候选目标点</span><br><span class="line">	</span><br><span class="line">	# 判断当前是否已接近目标区域</span><br><span class="line">	if (D_t &lt; δ_D)&#123;	# 如果 agent与目标的距离处于接近目标区域</span><br><span class="line">	</span><br><span class="line">		if(waypoint = global_goal)	# 若当前导航的子目标 waypoint 已经是 global_goal</span><br><span class="line">			reachedGlobalGoal = True	# 那么任务完成</span><br><span class="line">			</span><br><span class="line">		else&#123;	# 否则，进一步判断当前是否靠近全局目标</span><br><span class="line">		</span><br><span class="line">			if(d(p_t, g) &lt; δ)	# 如果当前位置p_t与目标g的距离d(p_t,g) 小于 δ</span><br><span class="line">				waypoint &lt;-- global_goal	# 那么就把当前的 waypoint 设置为 global_goal</span><br><span class="line">				</span><br><span class="line">			else	# 否则，从所有兴趣点中选择下一个最优子目标点</span><br><span class="line">				for i in POI	# 遍历POI中所有的兴趣点</span><br><span class="line">					caculate h(i) from (1) # 根据式子(1)计算每个兴趣点的启发值h(i)</span><br><span class="line">				waypoint &lt;-- POI_min(h(i))	# 将h(i)值最小对应的兴趣点作为新的 waypoint</span><br><span class="line">			end if</span><br><span class="line">		end if</span><br><span class="line">	end if</span><br><span class="line">	</span><br><span class="line">	Obtain an action from TD3	# 从 TD3 策略网络中获取当前动作,利用强化学习模型TD3预测最优动作</span><br><span class="line">	Perform action	# 执行该动作</span><br><span class="line">end while</span><br></pre></td></tr></table></figure>

<h4 id=""><a href="#" class="headerlink" title=""></a></h4></blockquote>
<h3 id="A-系统设置"><a href="#A-系统设置" class="headerlink" title="A.系统设置"></a>A.系统设置</h3><blockquote>
<h4 id="原作者系统配置："><a href="#原作者系统配置：" class="headerlink" title="原作者系统配置："></a>原作者系统配置：</h4><blockquote>
<h4 id="显卡：NVIDIA-GTX-1080"><a href="#显卡：NVIDIA-GTX-1080" class="headerlink" title="显卡：NVIDIA GTX 1080"></a>显卡：NVIDIA GTX 1080</h4><h4 id="运行内存：32G"><a href="#运行内存：32G" class="headerlink" title="运行内存：32G"></a>运行内存：32G</h4><h4 id="CPU：-Intel-Core-i7-6800K"><a href="#CPU：-Intel-Core-i7-6800K" class="headerlink" title="CPU： Intel Core i7-6800K"></a>CPU： Intel Core i7-6800K</h4></blockquote>
<h4 id="训练参数设置"><a href="#训练参数设置" class="headerlink" title="训练参数设置"></a>训练参数设置</h4><blockquote>
<h4 id="TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h"><a href="#TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h" class="headerlink" title="TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h"></a>TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h</h4><h4 id="每个训练回合在机器人到达目标、发生碰撞或执行了-500-步动作后结束"><a href="#每个训练回合在机器人到达目标、发生碰撞或执行了-500-步动作后结束" class="headerlink" title="每个训练回合在机器人到达目标、发生碰撞或执行了 500 步动作后结束"></a>每个训练回合在机器人到达目标、发生碰撞或执行了 500 步动作后结束</h4><h4 id="最大线速度-v-max-和最大角速度-ω-max-分别设置为-0-5-m-s-和-1rad-s"><a href="#最大线速度-v-max-和最大角速度-ω-max-分别设置为-0-5-m-s-和-1rad-s" class="headerlink" title="最大线速度 v_max 和最大角速度 ω_max 分别设置为 0.5 m&#x2F;s 和 1rad&#x2F;s"></a>最大线速度 v_max 和最大角速度 ω_max 分别设置为 0.5 m&#x2F;s 和 1rad&#x2F;s</h4><h4 id="延迟奖励在最后-n-10-步中更新，参数更新延迟设置为每-2-个回合"><a href="#延迟奖励在最后-n-10-步中更新，参数更新延迟设置为每-2-个回合" class="headerlink" title="延迟奖励在最后 n&#x3D;10 步中更新，参数更新延迟设置为每 2 个回合"></a>延迟奖励在最后 n&#x3D;10 步中更新，参数更新延迟设置为每 2 个回合</h4></blockquote>
<h4 id="训练在一个-10x10-米的模拟环境中进行，如图所示"><a href="#训练在一个-10x10-米的模拟环境中进行，如图所示" class="headerlink" title="训练在一个 10x10 米的模拟环境中进行，如图所示"></a>训练在一个 10x10 米的模拟环境中进行，如图所示</h4><p><img src="http://picbed.yanzu.tech/img/paper_read/1/4.jpg"></p>
<h4 id="训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图-a-、-b-和-c-所示，以实现训练数据的随机化。"><a href="#训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图-a-、-b-和-c-所示，以实现训练数据的随机化。" class="headerlink" title="训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图(a)、(b)和(c)所示，以实现训练数据的随机化。"></a>训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图(a)、(b)和(c)所示，以实现训练数据的随机化。</h4><h4 id="为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图-a-、-b-、-c-所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。"><a href="#为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图-a-、-b-、-c-所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。" class="headerlink" title="为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图 (a)、(b)、(c) 所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。"></a>为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图 (a)、(b)、(c) 所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。</h4><h4 id="ROS-中的-SLAM-Toolbox-软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位"><a href="#ROS-中的-SLAM-Toolbox-软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位" class="headerlink" title="ROS 中的 SLAM Toolbox 软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位"></a>ROS 中的 SLAM Toolbox 软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位</h4><h4 id="ROS的本地规划器包（TrajectoryPlanner）代替了神经网络"><a href="#ROS的本地规划器包（TrajectoryPlanner）代替了神经网络" class="headerlink" title="ROS的本地规划器包（TrajectoryPlanner）代替了神经网络"></a>ROS的本地规划器包（TrajectoryPlanner）代替了神经网络</h4><h4 id="目标驱动自主探索（GDAE-Goal-Driven-Autonomous-Exploration）"><a href="#目标驱动自主探索（GDAE-Goal-Driven-Autonomous-Exploration）" class="headerlink" title="目标驱动自主探索（GDAE, Goal-Driven Autonomous Exploration）"></a>目标驱动自主探索（GDAE, Goal-Driven Autonomous Exploration）</h4><h4 id="最近前沿探索策略（Nearest-Frontier-NF）"><a href="#最近前沿探索策略（Nearest-Frontier-NF）" class="headerlink" title="最近前沿探索策略（Nearest Frontier, NF）"></a>最近前沿探索策略（Nearest Frontier, NF）</h4><h4 id="目标驱动强化学习（GD-RL-Goal-Driven-Reinforcement-Learning）"><a href="#目标驱动强化学习（GD-RL-Goal-Driven-Reinforcement-Learning）" class="headerlink" title="目标驱动强化学习（GD-RL, Goal-Driven Reinforcement Learning）"></a>目标驱动强化学习（GD-RL, Goal-Driven Reinforcement Learning）</h4><h4 id="本地规划器自主探索（LP-AE，Local-Planner-Autonomous-Exploration）"><a href="#本地规划器自主探索（LP-AE，Local-Planner-Autonomous-Exploration）" class="headerlink" title="本地规划器自主探索（LP-AE，Local Planner Autonomous Exploration）"></a>本地规划器自主探索（LP-AE，Local Planner Autonomous Exploration）</h4><h4 id="路径规划器（PP，Path-Planner）它是基于-Dijkstra-的生成路径的方法"><a href="#路径规划器（PP，Path-Planner）它是基于-Dijkstra-的生成路径的方法" class="headerlink" title="路径规划器（PP，Path Planner）它是基于 Dijkstra 的生成路径的方法"></a>路径规划器（PP，Path Planner）它是基于 Dijkstra 的生成路径的方法</h4></blockquote>
<h3 id="B-定量实验"><a href="#B-定量实验" class="headerlink" title="B.定量实验"></a>B.定量实验</h3><h3 id="C-定性实验"><a href="#C-定性实验" class="headerlink" title="C.定性实验"></a>C.定性实验</h3></blockquote>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><blockquote>
<h4 id="基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）"><a href="#基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）" class="headerlink" title="基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）"></a>基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）</h4><h4 id="无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）"><a href="#无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）" class="headerlink" title="无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）"></a>无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）</h4><h4 id="系统有效结合了反应式的本地导航策略和全局导航策略"><a href="#系统有效结合了反应式的本地导航策略和全局导航策略" class="headerlink" title="系统有效结合了反应式的本地导航策略和全局导航策略"></a>系统有效结合了反应式的本地导航策略和全局导航策略</h4><h4 id="将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补"><a href="#将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补" class="headerlink" title="将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补"></a>将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补</h4><h4 id="系统的导航性能接近于基于已知地图路径规划器所得的最优解"><a href="#系统的导航性能接近于基于已知地图路径规划器所得的最优解" class="headerlink" title="系统的导航性能接近于基于已知地图路径规划器所得的最优解"></a>系统的导航性能接近于基于已知地图路径规划器所得的最优解</h4><h4 id="GDAE-系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳"><a href="#GDAE-系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳" class="headerlink" title="GDAE 系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳"></a>GDAE 系统依赖<strong>直接的传感器输入</strong>而非从不确定地图生成路径，因此在可靠性方面表现更佳</h4><h4 id="若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。"><a href="#若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。" class="headerlink" title="若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。"></a>若希望进一步<strong>泛化至不同类型的机器人</strong>，可以将<strong>机器人动力学作为神经网络的一个输入状态</strong>，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。</h4><h4 id="接下来的研究："><a href="#接下来的研究：" class="headerlink" title="接下来的研究："></a>接下来的研究：</h4><h4 id="引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物"><a href="#引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物" class="headerlink" title="引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物"></a>引入<strong>长短时记忆（LSTM）结构</strong>也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物</h4></blockquote>
<h3 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h3><blockquote>
<h4 id="两个量的定义"><a href="#两个量的定义" class="headerlink" title="两个量的定义"></a>两个量的定义</h4><h4 id="Episode："><a href="#Episode：" class="headerlink" title="Episode："></a>Episode：</h4><blockquote>
<h4 id="它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max-ep）"><a href="#它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max-ep）" class="headerlink" title="它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max_ep）"></a>它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max_ep）</h4><h4 id="它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变"><a href="#它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变" class="headerlink" title="它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变"></a>它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变</h4></blockquote>
<h4 id="Epoch："><a href="#Epoch：" class="headerlink" title="Epoch："></a>Epoch：</h4><blockquote>
<h4 id="执行评估之间的后续事件数（episode）或者时间步长（timesteps）"><a href="#执行评估之间的后续事件数（episode）或者时间步长（timesteps）" class="headerlink" title="执行评估之间的后续事件数（episode）或者时间步长（timesteps）"></a>执行评估之间的后续事件数（episode）或者时间步长（timesteps）</h4><h4 id="一个epoch运行5000个步骤step（对应代码中的-eval-freq-这个参数），步长为0-1秒，也就是说一个epoch大概要运行8分多钟"><a href="#一个epoch运行5000个步骤step（对应代码中的-eval-freq-这个参数），步长为0-1秒，也就是说一个epoch大概要运行8分多钟" class="headerlink" title="一个epoch运行5000个步骤step（对应代码中的 eval_freq 这个参数），步长为0.1秒，也就是说一个epoch大概要运行8分多钟"></a>一个epoch运行5000个步骤step（对应代码中的 eval_freq 这个参数），步长为0.1秒，也就是说一个epoch大概要运行8分多钟</h4></blockquote>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://yanzu.tech">yanzu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://yanzu.tech/2025/05/26/17/">http://yanzu.tech/2025/05/26/17/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://yanzu.tech" target="_blank">yanzu_blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="http://picbed.yanzu.tech/img/post_cover/p17.png" data-sites="wechat,qq,copy_link"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/05/22/16/" title="RL之路---第一弹"><img class="cover" src="http://picbed.yanzu.tech/img/post_cover/p16.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">RL之路---第一弹</div></div><div class="info-2"><div class="info-item-1">RL的基本数学原理—基本概念State 它是agent相对于当前环境的一个状态，如当前的坐标 (x,y)，速度、加速度等 所有的状态构成的一个集合称之为状态空间，如下图，s1~s9构成了一个状态空间，这里是2D的，那么状态主要就是location （x, y）   Action 在每一个状态下，都会有对应的一系列的动作Action，如2D平面上，在一个状态下可以采取的Action有前进、后退、左右移动、原地不动  所有的Action构成的一个集合就称之为动作空间 Action space Action 和 state 是相互依赖的，不同的状态下对应不同的动作$$A(s_i) &#x3D; {a_i}$$上式意为，在状态 s_i 下，可采取的动作 a_i  State transition 状态转换，在当前状态 s1 下，采取动作 a2(有概率采取动作 a2)，会转换到下一状态 s2，而这个下一状态 s2，其实是不确定的，它根据采取的动作而定，而且只是有概率转移到某个状态 s$$s_1 \xrightarrow{a_2} s_2$$状态转换定义了 agent...</div></div></div></a><a class="pagination-related" href="/2025/05/28/18/" title="科学上网？NO，是怒火中烧"><img class="cover" src="http://picbed.yanzu.tech/img/post_cover/p18.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">科学上网？NO，是怒火中烧</div></div><div class="info-2"><div class="info-item-1">老版本Ubuntu科学上网须知（boom！！！）必须记录下来 绝大多数版本的clash verge或者其他形式的软件都不支持切换内核为meta了为数不多的就是极低版本的，这里就是采用的 1.4.3 版本的先来个地址 https://github.com/clash-verge-rev/clash-verge-rev/releases?page=5  直接下载对应版本的 deb ，然后 dpkg 安装安装好之后，命令框同样会报错，提示说缺少依赖，叫 lib 什么来着先不管报错，先打开软件在设置这一栏，找到 Clash 内核 这行 点击齿轮，点击授权 Clash...</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">yanzu</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yanzu1024/yanzu1024.github.io"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/yanzu1024" target="_blank" title="Github"><i class="fab fa-github" style="color: #hdhfbb;"></i></a><a class="social-icon" href="mailto:472874946@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/648854222?spm_id_from=333.1007.0.0" target="_blank" title="Bilibili"><i class="fab fa-bilibili" style="color: #bilibili;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Goal-Driven-Autonomous-Exploration-Through-Deep-Reinforcement-Learning%E2%80%94%E9%80%9A%E8%BF%87%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E7%9B%AE%E6%A0%87%E9%A9%B1%E5%8A%A8%E7%9A%84%E8%87%AA%E4%B8%BB%E6%8E%A2%E7%B4%A2"><span class="toc-number">1.</span> <span class="toc-text">Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning—通过深度强化学习实现目标驱动的自主探索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.0.1.</span> <span class="toc-text">摘要</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88DRL%EF%BC%89%E7%9A%84%E8%87%AA%E4%B8%BB%E5%AF%BC%E8%88%AA%E7%B3%BB%E7%BB%9F%EF%BC%8C%E7%94%A8%E4%BA%8E%E5%9C%A8%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E4%B8%AD%E8%BF%9B%E8%A1%8C%E7%9B%AE%E6%A0%87%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%8E%A2%E7%B4%A2%E3%80%82%E7%B3%BB%E7%BB%9F%E4%BB%8E%E7%8E%AF%E5%A2%83%E4%B8%AD%E8%8E%B7%E5%8F%96%E5%8F%AF%E8%83%BD%E7%9A%84%E5%AF%BC%E8%88%AA%E6%96%B9%E5%90%91%E7%9A%84%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%88POI%EF%BC%89%EF%BC%8C%E5%B9%B6%E5%9F%BA%E4%BA%8E%E5%8F%AF%E7%94%A8%E6%95%B0%E6%8D%AE%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AA%E6%9C%80%E4%BC%98%E7%9A%84%E8%88%AA%E8%B7%AF%E7%82%B9%E3%80%82%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%A0%B9%E6%8D%AE%E8%BF%99%E4%BA%9B%E8%88%AA%E8%B7%AF%E7%82%B9%E8%A2%AB%E5%BC%95%E5%AF%BC%E5%90%91%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%EF%BC%8C%E7%BC%93%E8%A7%A3%E4%BA%86%E5%8F%8D%E5%BA%94%E5%BC%8F%E5%AF%BC%E8%88%AA%E4%B8%AD%E7%9A%84%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98%E3%80%82%E9%9A%8F%E5%90%8E%EF%BC%8C%E5%9C%A8%E4%BB%BF%E7%9C%9F%E7%8E%AF%E5%A2%83%E4%B8%AD%E9%80%9A%E8%BF%87DRL%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8E%E5%B1%80%E9%83%A8%E5%AF%BC%E8%88%AA%E7%9A%84%E8%BF%90%E5%8A%A8%E7%AD%96%E7%95%A5%E3%80%82%E6%88%91%E4%BB%AC%E5%BC%80%E5%8F%91%E4%BA%86%E4%B8%80%E4%B8%AA%E5%AF%BC%E8%88%AA%E7%B3%BB%E7%BB%9F%EF%BC%8C%E5%B0%86%E8%AF%A5%E5%AD%A6%E4%B9%A0%E5%88%B0%E7%9A%84%E7%AD%96%E7%95%A5%E9%9B%86%E6%88%90%E5%88%B0%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92%E7%B3%BB%E7%BB%9F%E4%B8%AD%EF%BC%8C%E4%BD%9C%E4%B8%BA%E5%B1%80%E9%83%A8%E5%AF%BC%E8%88%AA%E5%B1%82%EF%BC%8C%E7%94%A8%E4%BA%8E%E9%A9%B1%E5%8A%A8%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%9C%A8%E8%88%AA%E8%B7%AF%E7%82%B9%E4%B9%8B%E9%97%B4%E7%A7%BB%E5%8A%A8%EF%BC%8C%E6%9C%80%E7%BB%88%E5%88%B0%E8%BE%BE%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E3%80%82%E6%95%B4%E4%B8%AA%E8%87%AA%E4%B8%BB%E5%AF%BC%E8%88%AA%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%97%A0%E9%9C%80%E4%BB%BB%E4%BD%95%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86%EF%BC%8C%E5%90%8C%E6%97%B6%E5%9C%A8%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%A7%BB%E5%8A%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%BC%9A%E8%AE%B0%E5%BD%95%E7%8E%AF%E5%A2%83%E5%9C%B0%E5%9B%BE%E3%80%82%E5%AE%9E%E9%AA%8C%E8%A1%A8%E6%98%8E%EF%BC%8C%E8%AF%A5%E6%96%B9%E6%B3%95%E5%9C%A8%E6%97%A0%E9%9C%80%E5%9C%B0%E5%9B%BE%E6%88%96%E5%85%88%E9%AA%8C%E4%BF%A1%E6%81%AF%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%9C%A8%E5%A4%8D%E6%9D%82%E7%9A%84%E9%9D%99%E6%80%81%E5%92%8C%E5%8A%A8%E6%80%81%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9B%B8%E8%BE%83%E4%BA%8E%E5%85%B6%E4%BB%96%E6%8E%A2%E7%B4%A2%E6%96%B9%E6%B3%95%E5%85%B7%E6%9C%89%E4%BC%98%E5%8A%BF%E3%80%82"><span class="toc-number">1.0.1.1.</span> <span class="toc-text">本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E4%B8%BB%E5%AF%BC%E8%88%AA%E7%B3%BB%E7%BB%9F"><span class="toc-number">1.0.1.2.</span> <span class="toc-text">自主导航系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8%E4%BA%8E%E9%80%9A%E8%BF%87DRL%E5%AF%B9%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E8%BF%9B%E8%A1%8C%E7%9B%AE%E6%A0%87%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%8E%A2%E7%B4%A2"><span class="toc-number">1.0.1.3.</span> <span class="toc-text">用于通过DRL对未知环境进行目标驱动的探索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E5%8F%AF%E8%83%BD%E5%AF%BC%E8%88%AA%E6%96%B9%E5%90%91%E7%9A%84%E5%85%B4%E8%B6%A3%E7%82%B9-POI"><span class="toc-number">1.0.1.4.</span> <span class="toc-text">获取可能导航方向的兴趣点(POI)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E5%8F%AF%E7%94%A8%E6%95%B0%E6%8D%AE%E9%80%89%E6%8B%A9%E6%9C%80%E4%BD%B3%E8%88%AA%E8%B7%AF%E7%82%B9"><span class="toc-number">1.0.1.5.</span> <span class="toc-text">根据可用数据选择最佳航路点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%93%E8%A7%A3%E5%8F%8D%E5%BA%94%E5%BC%8F%E5%AF%BC%E8%88%AA%E4%B8%AD%E7%9A%84%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98"><span class="toc-number">1.0.1.6.</span> <span class="toc-text">缓解反应式导航中的局部最优问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%87%E7%94%A8%E7%9A%84%E6%98%AFTD3%E7%AE%97%E6%B3%95%EF%BC%88%E5%8F%8C%E5%BB%B6%E8%BF%9F%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%89"><span class="toc-number">1.0.1.7.</span> <span class="toc-text">采用的是TD3算法（双延迟深度确定性策略梯度）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%83%E6%98%AF%E5%9C%A8DDPG%E7%AE%97%E6%B3%95%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E8%BF%9B%E8%A1%8C%E7%9A%84%E6%89%A9%E5%B1%95"><span class="toc-number">1.0.1.8.</span> <span class="toc-text">它是在DDPG算法的基础上进行的扩展</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DDPG%EF%BC%88Deep-Deterministic-Policy-Gradient%EF%BC%89%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%9A%E5%AE%83%E6%98%AF%E4%B8%80%E7%A7%8D%E5%90%8C%E6%97%B6%E5%AD%A6%E4%B9%A0Q%E5%87%BD%E6%95%B0%E5%92%8C%E7%AD%96%E7%95%A5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AE%83%E4%BD%BF%E7%94%A8%E9%9D%9E%E7%AD%96%E7%95%A5%E6%95%B0%E6%8D%AE%E5%92%8C%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B%E6%9D%A5%E5%AD%A6%E4%B9%A0Q%E5%87%BD%E6%95%B0%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8Q%E5%87%BD%E6%95%B0%E6%9D%A5%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5"><span class="toc-number">1.0.1.9.</span> <span class="toc-text">DDPG（Deep Deterministic Policy Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%EF%BC%9Ahttps-spinningup-openai-com-en-latest-algorithms-ddpg-html-background"><span class="toc-number">1.0.1.10.</span> <span class="toc-text">详细介绍：https:&#x2F;&#x2F;spinningup.openai.com&#x2F;en&#x2F;latest&#x2F;algorithms&#x2F;ddpg.html#background</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DDPG-%E5%AF%B9%E8%B6%85%E5%8F%82%E6%95%B0%E5%92%8C%E5%85%B6%E4%BB%96%E8%B0%83%E4%BC%98%E6%89%8B%E6%AE%B5%E9%9D%9E%E5%B8%B8%E6%95%8F%E6%84%9F%EF%BC%8C%E8%A1%A8%E7%8E%B0%E5%B8%B8%E5%B8%B8%E4%B8%8D%E7%A8%B3%E5%AE%9A%EF%BC%8C%E5%AE%83%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B8%B8%E8%A7%81%E5%A4%B1%E8%B4%A5%E6%A8%A1%E5%BC%8F%E6%98%AF%EF%BC%9A%E6%89%80%E5%AD%A6%E4%B9%A0%E7%9A%84-Q-%E5%87%BD%E6%95%B0%E4%BC%9A%E4%B8%A5%E9%87%8D%E9%AB%98%E4%BC%B0-Q-%E5%80%BC%EF%BC%8C%E8%BF%9B%E8%80%8C%E5%AF%BC%E8%87%B4%E7%AD%96%E7%95%A5%E5%B4%A9%E6%BA%83%EF%BC%8C%E5%9B%A0%E4%B8%BA%E7%AD%96%E7%95%A5%E4%BC%9A%E5%88%A9%E7%94%A8-Q-%E5%87%BD%E6%95%B0%E4%B8%AD%E7%9A%84%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.0.1.11.</span> <span class="toc-text">DDPG 对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的 Q 函数会严重高估 Q 值，进而导致策略崩溃，因为策略会利用 Q 函数中的误差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%8C%E5%8F%8C%E5%BB%B6%E8%BF%9F%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%EF%BC%88TD3%EF%BC%89%E5%BC%95%E5%85%A5%E4%B8%89%E4%B8%AA%E5%85%B3%E9%94%AE%E6%8A%80%E5%B7%A7%E6%9D%A5%E8%A7%A3%E5%86%B3DDPG%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A"><span class="toc-number">1.0.1.12.</span> <span class="toc-text">而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%88%AA%E6%96%AD%E7%9A%84%E5%8F%8CQ%E5%AD%A6%E4%B9%A0%EF%BC%9ATD3-%E5%90%8C%E6%97%B6%E5%AD%A6%E4%B9%A0%E4%B8%A4%E4%B8%AA-Q-%E5%87%BD%E6%95%B0%EF%BC%88%E5%9B%A0%E6%AD%A4%E7%A7%B0%E4%B8%BA%E2%80%9C%E5%8F%8C%E2%80%9DQ%EF%BC%89%EF%BC%8C%E5%9C%A8%E8%AE%A1%E7%AE%97%E8%B4%9D%E5%B0%94%E6%9B%BC%E8%AF%AF%E5%B7%AE%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%AD%E7%9A%84%E7%9B%AE%E6%A0%87%E5%80%BC%E6%97%B6%EF%BC%8C%E5%8F%96%E4%B8%A4%E4%B8%AAQ%E5%80%BC%E4%B8%AD%E8%BE%83%E5%B0%8F%E7%9A%84%E4%B8%80%E4%B8%AA%EF%BC%8C%E4%BB%A5%E9%99%8D%E4%BD%8E%E8%BF%87%E4%BC%B0%E8%AE%A1%E7%9A%84%E9%A3%8E%E9%99%A9"><span class="toc-number">1.0.1.13.</span> <span class="toc-text">截断的双Q学习：TD3 同时学习两个 Q 函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BB%B6%E8%BF%9F%E7%9A%84%E7%AD%96%E7%95%A5%E6%9B%B4%E6%96%B0%EF%BC%9ATD3-%E4%B8%AD%EF%BC%8C%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C%EF%BC%88Actor%EF%BC%89%E5%92%8C%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9B%B4%E6%96%B0%E9%A2%91%E7%8E%87%E4%BD%8E%E4%BA%8E-Q-%E5%87%BD%E6%95%B0%E7%9A%84%E6%9B%B4%E6%96%B0%E9%A2%91%E7%8E%87%EF%BC%8C%E5%BB%BA%E8%AE%AE%E6%98%AF%EF%BC%8C%E6%9B%B4%E6%96%B0%E4%B8%A4%E6%AC%A1-Q-%E7%BD%91%E7%BB%9C%EF%BC%8C%E5%8F%AA%E6%9B%B4%E6%96%B0-%E4%B8%80%E6%AC%A1%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C%EF%BC%8C%E4%BB%A5%E6%8F%90%E9%AB%98%E8%AE%AD%E7%BB%83%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="toc-number">1.0.1.14.</span> <span class="toc-text">延迟的策略更新：TD3 中，策略网络（Actor）和目标网络的更新频率低于 Q 函数的更新频率，建议是，更新两次 Q 网络，只更新 一次策略网络，以提高训练稳定性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E7%AD%96%E7%95%A5%E5%B9%B3%E6%BB%91%EF%BC%9ATD3-%E5%9C%A8%E8%AE%A1%E7%AE%97%E7%9B%AE%E6%A0%87-Q-%E5%80%BC%E6%97%B6%EF%BC%8C%E4%BC%9A%E5%9C%A8%E7%9B%AE%E6%A0%87%E5%8A%A8%E4%BD%9C%E4%B8%8A%E6%B7%BB%E5%8A%A0%E5%99%AA%E5%A3%B0%EF%BC%8C%E4%B8%80%E8%88%AC%E6%98%AF%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0%EF%BC%8C%E4%BD%BF%E5%BE%97%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4%E5%86%85%E7%9A%84-Q-%E5%80%BC%E6%9B%B4%E5%B9%B3%E6%BB%91%EF%BC%8C%E4%BB%8E%E8%80%8C%E9%98%B2%E6%AD%A2%E7%AD%96%E7%95%A5%E5%8E%BB%E2%80%9C%E9%92%BB%E7%A9%BA%E5%AD%90%E2%80%9D%E5%88%A9%E7%94%A8-Q-%E5%87%BD%E6%95%B0%E4%B8%AD%E7%9A%84%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.0.1.15.</span> <span class="toc-text">目标策略平滑：TD3 在计算目标 Q 值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的 Q 值更平滑，从而防止策略去“钻空子”利用 Q 函数中的误差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%EF%BC%9Ahttps-spinningup-openai-com-en-latest-algorithms-td3-html"><span class="toc-number">1.0.1.16.</span> <span class="toc-text">详细介绍：https:&#x2F;&#x2F;spinningup.openai.com&#x2F;en&#x2F;latest&#x2F;algorithms&#x2F;td3.html</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.0.2.</span> <span class="toc-text">介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E5%85%A8%E8%87%AA%E4%B8%BB%E7%9A%84%E7%9B%AE%E6%A0%87%E9%A9%B1%E5%8A%A8%E6%8E%A2%E7%B4%A2%E6%98%AF%E4%B8%80%E4%B8%AA%E5%8C%85%E5%90%AB%E4%B8%A4%E4%B8%AA%E6%96%B9%E9%9D%A2%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.0.2.1.</span> <span class="toc-text">完全自主的目标驱动探索是一个包含两个方面的问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A6%96%E5%85%88%EF%BC%8C%E6%8E%A2%E7%B4%A2%E6%9C%BA%E5%99%A8%E4%BA%BA%E9%9C%80%E8%A6%81%E5%86%B3%E7%AD%96%E5%8E%BB%E5%93%AA%E4%B8%80%E4%B8%AA%E6%96%B9%E5%90%91%EF%BC%8C%E4%BB%A5%E6%9C%80%E5%A4%A7%E5%8F%AF%E8%83%BD%E6%80%A7%E5%88%B0%E8%BE%BE%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E3%80%82%E5%9C%A8%E6%B2%A1%E6%9C%89%E5%85%88%E9%AA%8C%E4%BF%A1%E6%81%AF%E6%88%96%E6%97%A0%E6%B3%95%E8%A7%82%E6%B5%8B%E5%88%B0%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E7%B3%BB%E7%BB%9F%E9%9C%80%E8%A6%81%E7%9B%B4%E6%8E%A5%E4%BB%8E%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E4%B8%AD%E6%8C%87%E7%A4%BA%E5%8F%AF%E8%83%BD%E7%9A%84%E5%AF%BC%E8%88%AA%E6%96%B9%E5%90%91%E3%80%82%E4%BB%8E%E8%BF%99%E4%BA%9B%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%88POI%EF%BC%89%E4%B8%AD%EF%BC%8C%E9%9C%80%E8%A6%81%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AA%E6%9C%80%E4%BC%98%E7%82%B9%E4%BD%9C%E4%B8%BA%E8%88%AA%E8%B7%AF%E7%82%B9%EF%BC%8C%E4%BB%A5%E6%9C%80%E4%BC%98%E7%9A%84%E6%96%B9%E5%BC%8F%E5%BC%95%E5%AF%BC%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8E%A5%E8%BF%91%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E3%80%82"><span class="toc-number">1.0.2.2.</span> <span class="toc-text">首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E6%AC%A1%EF%BC%8C%E5%9C%A8%E4%B8%8D%E4%BE%9D%E8%B5%96%E5%9C%B0%E5%9B%BE%E6%95%B0%E6%8D%AE%E7%9A%84%E5%89%8D%E6%8F%90%E4%B8%8B%EF%BC%8C%E8%BF%98%E9%9C%80%E8%A6%81%E8%8E%B7%E5%BE%97%E4%B8%80%E7%A7%8D%E9%80%82%E7%94%A8%E4%BA%8E%E4%B8%8D%E7%A1%AE%E5%AE%9A%E7%8E%AF%E5%A2%83%E7%9A%84%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%BF%90%E5%8A%A8%E7%AD%96%E7%95%A5%E3%80%82"><span class="toc-number">1.0.2.3.</span> <span class="toc-text">其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%99%E5%B0%B1%E5%BC%95%E5%85%A5%E4%BA%86DRL"><span class="toc-number">1.0.2.4.</span> <span class="toc-text">这就引入了DRL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%86%E6%98%AFDRL%E5%AD%98%E5%9C%A8%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%EF%BC%9ADRL%E5%85%B7%E6%9C%89%E5%8F%8D%E5%BA%94%E6%80%A7%E7%89%B9%E5%BE%81%E4%B8%94%E7%BC%BA%E4%B9%8F%E5%85%A8%E5%B1%80%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%85%B6%E5%AE%B9%E6%98%93%E9%99%B7%E5%85%A5%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98%E5%B0%A4%E5%85%B6%E6%98%AF%E5%9C%A8%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%AF%BC%E8%88%AA%E4%BB%BB%E5%8A%A1%E4%B8%AD%E8%A1%A8%E7%8E%B0%E5%B0%A4%E4%B8%BA%E6%98%8E%E6%98%BE%E3%80%82"><span class="toc-number">1.0.2.5.</span> <span class="toc-text">但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DRL%E7%9A%84%E5%8F%8D%E5%BA%94%E6%80%A7%E7%89%B9%E5%BE%81"><span class="toc-number">1.0.2.6.</span> <span class="toc-text">DRL的反应性特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DRL%E4%B8%AD%E7%9A%84%E5%A4%A7%E5%A4%9A%E7%AD%96%E7%95%A5%E9%83%BD%E6%98%AF%E5%9F%BA%E4%BA%8E%E5%BD%93%E5%89%8D%E7%8A%B6%E6%80%81%E5%81%9A%E5%87%BA%E5%86%B3%E7%AD%96%E7%9A%84%EF%BC%8C%E4%B9%9F%E5%8D%B3-reactive-policy%EF%BC%88%E5%8F%8D%E5%BA%94%E5%BC%8F%E7%AD%96%E7%95%A5%EF%BC%89%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%85%B7%E6%9C%89%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7"><span class="toc-number">1.0.2.7.</span> <span class="toc-text">DRL中的大多策略都是基于当前状态做出决策的，也即 reactive policy（反应式策略），也就是具有马尔可夫性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%99%E4%BC%9A%E5%AF%BC%E8%87%B4%EF%BC%9A"><span class="toc-number">1.0.2.8.</span> <span class="toc-text">这会导致：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#agent%E5%8F%AA%E4%BE%9D%E6%8D%AE%E5%BD%93%E5%89%8D%E8%A7%82%E6%B5%8B%EF%BC%88%E6%91%84%E5%83%8F%E5%A4%B4%E5%9B%BE%E5%83%8F%E3%80%81%E9%9B%B7%E8%BE%BE%E6%89%AB%E6%8F%8F%EF%BC%89%E6%9D%A5%E5%86%B3%E5%AE%9A%E6%88%96%E5%88%A4%E6%96%AD%E4%B8%8B%E4%B8%80%E5%8A%A8%E4%BD%9C"><span class="toc-number">1.0.2.9.</span> <span class="toc-text">agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B2%A1%E6%9C%89%E5%85%A8%E5%B1%80%E5%9C%B0%E5%9B%BE%E6%88%96%E4%BB%BB%E5%8A%A1%E7%BB%93%E6%9E%9C%E7%9A%84%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%EF%BC%8C%E7%BC%BA%E5%B0%91%E6%9B%B4%E9%AB%98%E5%B1%82%E6%AC%A1%E7%9A%84%E8%AE%A1%E5%88%92%E6%88%96%E5%86%B3%E7%AD%96%E8%83%BD%E5%8A%9B"><span class="toc-number">1.0.2.10.</span> <span class="toc-text">没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E4%BD%9C%E7%9A%84%E9%80%89%E6%8B%A9%E5%BE%80%E5%BE%80%E5%9F%BA%E4%BA%8E%E7%9F%AD%E6%9C%9F%E5%A5%96%E5%8A%B1%E6%9C%80%E5%A4%A7%E5%8C%96%EF%BC%8C%E8%80%8C%E9%9D%9E%E9%95%BF%E6%9C%9F%E5%85%A8%E5%B1%80%E6%9C%80%E4%BC%98"><span class="toc-number">1.0.2.11.</span> <span class="toc-text">动作的选择往往基于短期奖励最大化，而非长期全局最优</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%99%E7%A7%8D%E7%AD%96%E7%95%A5%E9%80%82%E7%94%A8%E4%BA%8E%E5%BF%AB%E9%80%9F%E5%8F%8D%E5%BA%94%E3%80%81%E5%AE%9E%E6%97%B6%E9%81%BF%E9%9A%9C%E7%AD%89%E5%9C%BA%E6%99%AF"><span class="toc-number">1.0.2.12.</span> <span class="toc-text">这种策略适用于快速反应、实时避障等场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E4%BF%A1%E6%81%AF%E7%9A%84%E7%BC%BA%E4%B9%8F"><span class="toc-number">1.0.2.13.</span> <span class="toc-text">全局信息的缺乏</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E5%A4%9A%E6%95%B0DRL%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%BE%93%E5%85%A5%E9%83%BD%E6%98%AF%E5%B1%80%E9%83%A8%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%EF%BC%88%E5%BD%93%E5%89%8D%E4%BD%8D%E7%BD%AE%E7%9A%84%E6%91%84%E5%83%8F%E5%A4%B4%E5%9B%BE%E5%83%8F%E3%80%81%E9%9B%B7%E8%BE%BE%E6%89%AB%E6%8F%8F%EF%BC%89%EF%BC%8C%E5%9B%A0%E6%AD%A4DRL%E4%B9%9F%E5%8F%AA%E8%83%BD%E6%84%9F%E7%9F%A5%E5%B1%80%E9%83%A8%E7%8E%AF%E5%A2%83%E3%80%82%E5%8F%A6%E5%A4%96%EF%BC%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0RL%E6%9C%AC%E8%BA%AB%E5%B0%B1%E6%98%AF%E4%B8%80%E7%A7%8D%E5%B1%80%E9%83%A8%E7%AD%96%E7%95%A5%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AE%83%E6%9B%B4%E9%80%82%E5%90%88%E8%A7%A3%E5%86%B3%E7%9A%84%E6%98%AF%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8BMDP%E9%97%AE%E9%A2%98%E3%80%82%E8%BF%98%E6%9C%89%E5%B0%B1%E6%98%AF%EF%BC%8CDRL%E7%9A%84%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%E5%B0%B1%E6%98%AF%E6%9C%80%E5%A4%A7%E5%8C%96%E7%B4%AF%E8%AE%A1%E5%A5%96%E5%8A%B1%EF%BC%8C%E8%80%8C%E9%9D%9E%E6%9E%84%E5%BB%BA%E8%AE%A4%E7%9F%A5%E5%9C%B0%E5%9B%BE%E3%80%82"><span class="toc-number">1.0.2.14.</span> <span class="toc-text">大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98"><span class="toc-number">1.0.2.15.</span> <span class="toc-text">局部最优问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E5%9C%A8%E5%BD%93%E5%89%8D%E7%8A%B6%E6%80%81%E4%B8%8B%E9%80%89%E6%8B%A9%E4%BA%86%E4%B8%80%E4%B8%AA%E7%9C%8B%E4%BC%BC%E6%94%B6%E7%9B%8A%E6%9C%80%E5%A4%A7%EF%BC%88%E6%88%96%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%EF%BC%89%E7%9A%84%E8%A1%8C%E4%B8%BA%EF%BC%8C%E4%BD%86%E8%BF%99%E4%B8%AA%E9%80%89%E6%8B%A9%E5%8D%B4%E5%8F%AF%E8%83%BD%E9%98%BB%E7%A2%8D%E4%BA%86%E5%85%B6%E9%80%9A%E5%90%91%E7%9C%9F%E6%AD%A3%E5%85%A8%E5%B1%80%E6%9C%80%E4%BC%98%E8%A7%A3%E7%9A%84%E8%B7%AF%E5%BE%84%E3%80%82"><span class="toc-number">1.0.2.16.</span> <span class="toc-text">系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E5%AE%8C%E5%85%A8%E8%87%AA%E4%B8%BB%E7%9A%84%E6%8E%A2%E7%B4%A2%E7%B3%BB%E7%BB%9F%EF%BC%8C%E7%94%A8%E4%BA%8E%E5%9C%A8%E6%97%A0%E9%9C%80%E4%BA%BA%E5%B7%A5%E6%8E%A7%E5%88%B6%E6%88%96%E7%8E%AF%E5%A2%83%E5%85%88%E9%AA%8C%E4%BF%A1%E6%81%AF%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E5%AE%9E%E7%8E%B0%E5%AF%BC%E8%88%AA%E8%87%B3%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E3%80%82"><span class="toc-number">1.0.2.17.</span> <span class="toc-text">本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%A5%E7%B3%BB%E7%BB%9F%E4%BB%8E%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%91%A8%E5%9B%B4%E7%9A%84%E5%B1%80%E9%83%A8%E7%8E%AF%E5%A2%83%E4%B8%AD%E6%8F%90%E5%8F%96%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%88POI%EF%BC%89%EF%BC%8C%E5%AF%B9%E5%85%B6%E8%BF%9B%E8%A1%8C%E8%AF%84%E4%BC%B0%EF%BC%8C%E5%B9%B6%E4%BB%8E%E4%B8%AD%E9%80%89%E5%8F%96%E4%B8%80%E4%B8%AA%E4%BD%9C%E4%B8%BA%E8%88%AA%E8%B7%AF%E7%82%B9%EF%BC%88waypoint%EF%BC%89%E3%80%82"><span class="toc-number">1.0.2.18.</span> <span class="toc-text">该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%99%E4%BA%9B%E8%88%AA%E8%B7%AF%E7%82%B9%E7%94%A8%E4%BA%8E%E5%BC%95%E5%AF%BC%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88DRL%EF%BC%89%E7%9A%84%E8%BF%90%E5%8A%A8%E7%AD%96%E7%95%A5%E6%9C%9D%E5%90%91%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%EF%BC%8C%E4%BB%8E%E8%80%8C%E7%BC%93%E8%A7%A3%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98%E3%80%82"><span class="toc-number">1.0.2.19.</span> <span class="toc-text">这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%BE%9D%E6%8D%AE%E8%AF%A5%E7%AD%96%E7%95%A5%E8%BF%9B%E8%A1%8C%E8%BF%90%E5%8A%A8%EF%BC%8C%E6%97%A0%E9%9C%80%E5%AF%B9%E5%91%A8%E5%9B%B4%E7%8E%AF%E5%A2%83%E8%BF%9B%E8%A1%8C%E5%AE%8C%E6%95%B4%E5%BB%BA%E5%9B%BE%E3%80%82"><span class="toc-number">1.0.2.20.</span> <span class="toc-text">机器人依据该策略进行运动，无需对周围环境进行完整建图。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#POI"><span class="toc-number">1.0.2.21.</span> <span class="toc-text">POI</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%99%E9%87%8C%E6%89%80%E8%AF%B4%E7%9A%84-POI-%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%8C%E6%98%AF%E6%8C%87%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%8E%AF%E5%A2%83%E4%B8%AD%E9%82%A3%E4%BA%9B%E5%85%B7%E6%9C%89%E5%AF%BC%E8%88%AA%E4%BB%B7%E5%80%BC%E7%9A%84%E7%89%B9%E5%AE%9A%E7%82%B9%E6%88%96%E4%BD%8D%E7%BD%AE%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%82%B9%E9%80%9A%E5%B8%B8%E6%98%AF%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%86%B3%E7%AD%96%E2%80%9C%E5%8E%BB%E5%93%AA%E5%84%BF%E2%80%9D%E7%9A%84%E5%80%99%E9%80%89%E7%9B%AE%E6%A0%87%E3%80%82%E5%AE%83%E4%BB%AC%E5%B8%AE%E5%8A%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%9C%A8%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83%E9%87%8C%E9%80%89%E6%8B%A9%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%A1%8C%E5%8A%A8%E6%96%B9%E5%90%91%EF%BC%8C%E7%89%B9%E5%88%AB%E6%98%AF%E5%9C%A8%E6%B2%A1%E6%9C%89%E5%AE%8C%E6%95%B4%E5%9C%B0%E5%9B%BE%E4%BF%A1%E6%81%AF%E6%97%B6%E3%80%82POI-%E5%8F%AF%E4%BB%A5%E4%BD%9C%E4%B8%BA%E5%AF%BC%E8%88%AA%E8%B7%AF%E5%BE%84%E7%9A%84%E5%80%99%E9%80%89%E7%82%B9%EF%BC%8C%E4%B9%9F%E5%8D%B3%E4%B8%AD%E9%97%B4%E8%88%AA%E8%B7%AF%E7%82%B9waypoint%E3%80%82"><span class="toc-number">1.0.2.22.</span> <span class="toc-text">这里所说的 POI 兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI 可以作为导航路径的候选点，也即中间航路点waypoint。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#POI%E7%9A%84%E7%A1%AE%E5%AE%9A"><span class="toc-number">1.0.2.23.</span> <span class="toc-text">POI的确定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E5%85%89%E8%AF%BB%E6%95%B0%E9%97%B4%E9%9A%99%E4%BA%A7%E7%94%9F%E7%9A%84POI%EF%BC%9A%E5%BD%93%E8%BF%9E%E7%BB%AD%E4%B8%A4%E6%9D%9F%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E6%B5%8B%E8%B7%9D%E6%95%B0%E5%80%BC%E5%B7%AE%E8%B7%9D%E8%BE%83%E5%A4%A7%E6%97%B6%EF%BC%8C%E8%AF%B4%E6%98%8E%E5%8F%AF%E8%83%BD%E6%9C%89%E4%B8%80%E4%B8%AA%E5%BC%80%E5%8F%A3%E6%88%96%E9%80%9A%E9%81%93%EF%BC%8C%E8%BF%99%E4%B8%AA%E4%BD%8D%E7%BD%AE%E5%B0%B1%E8%A2%AB%E8%A7%86%E4%BD%9C%E4%B8%80%E4%B8%AA-POI%E3%80%82"><span class="toc-number">1.0.2.24.</span> <span class="toc-text">激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个 POI。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E6%95%B0%E5%80%BC%E6%BF%80%E5%85%89%E8%AF%BB%E6%95%B0%E4%BA%A7%E7%94%9F%E7%9A%84POI%EF%BC%9A%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E8%B6%85%E5%87%BA%E6%B5%8B%E9%87%8F%E8%8C%83%E5%9B%B4%E6%97%B6%E8%BF%94%E5%9B%9E%E9%9D%9E%E6%95%B0%E5%80%BC%EF%BC%8C%E4%BB%A3%E8%A1%A8%E8%BF%9C%E5%A4%84%E6%9C%89%E8%87%AA%E7%94%B1%E7%A9%BA%E9%97%B4%EF%BC%8C%E8%BF%99%E4%BA%9B%E8%BE%B9%E7%BC%98%E4%BD%8D%E7%BD%AE%E4%B9%9F%E8%A2%AB%E6%A0%87%E8%AE%B0%E4%B8%BA%E5%85%B4%E8%B6%A3%E7%82%B9%E3%80%82"><span class="toc-number">1.0.2.25.</span> <span class="toc-text">非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%BC%E8%88%AA%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%9A%E5%B7%A6%E4%BE%A7%E5%B1%95%E7%A4%BA%E4%BA%86%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%AE%BE%E7%BD%AE%EF%BC%8C%E4%B8%AD%E9%97%B4%E5%92%8C%E5%8F%B3%E4%BE%A7%E5%88%86%E5%88%AB%E5%8F%AF%E8%A7%86%E5%8C%96%E4%BA%86%E5%85%A8%E5%B1%80%E5%AF%BC%E8%88%AA%E4%B8%8E%E5%B1%80%E9%83%A8%E5%AF%BC%E8%88%AA%E7%9A%84%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86%E5%8F%8A%E5%85%B6%E6%95%B0%E6%8D%AE%E6%B5%81%E3%80%82"><span class="toc-number">1.0.2.26.</span> <span class="toc-text">导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.0.3.</span> <span class="toc-text">实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%BC%E8%88%AA%E7%BB%93%E6%9E%84%E5%88%86%E4%B8%A4%E9%83%A8%E5%88%86%EF%BC%9A"><span class="toc-number">1.0.3.1.</span> <span class="toc-text">导航结构分两部分：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B7%E6%9C%89%E6%9C%80%E4%BC%98%E8%88%AA%E8%B7%AF%E7%82%B9%E9%80%89%E6%8B%A9%E6%9C%BA%E5%88%B6%E7%9A%84%E5%85%A8%E5%B1%80%E5%AF%BC%E8%88%AA%E4%B8%8E%E5%BB%BA%E5%9B%BE%E6%A8%A1%E5%9D%97"><span class="toc-number">1.0.3.2.</span> <span class="toc-text">具有最优航路点选择机制的全局导航与建图模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B1%80%E9%83%A8%E5%AF%BC%E8%88%AA%E6%A8%A1%E5%9D%97"><span class="toc-number">1.0.3.3.</span> <span class="toc-text">基于深度强化学习的局部导航模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E9%A6%96%E5%85%88%E4%BB%8E%E7%8E%AF%E5%A2%83%E4%B8%AD%E6%8F%90%E5%8F%96%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%88POI%EF%BC%89%EF%BC%8C%E5%B9%B6%E4%BE%9D%E6%8D%AE%E8%AE%BE%E5%AE%9A%E7%9A%84%E8%AF%84%E4%BC%B0%E6%A0%87%E5%87%86%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AA%E6%9C%80%E4%BC%98%E8%88%AA%E8%B7%AF%E7%82%B9%E3%80%82"><span class="toc-number">1.0.3.4.</span> <span class="toc-text">系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E6%AF%8F%E4%B8%80%E4%B8%AA%E5%AF%BC%E8%88%AA%E6%AD%A5%E9%AA%A4%E4%B8%AD%EF%BC%8C%E7%B3%BB%E7%BB%9F%E4%BC%9A%E5%B0%86%E8%AF%A5%E8%88%AA%E8%B7%AF%E7%82%B9%E4%BB%A5%E7%9B%B8%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BD%93%E5%89%8D%E4%BD%8D%E7%BD%AE%E4%B8%8E%E6%9C%9D%E5%90%91%E7%9A%84%E6%9E%81%E5%9D%90%E6%A0%87%E5%BD%A2%E5%BC%8F%E8%BE%93%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%82"><span class="toc-number">1.0.3.5.</span> <span class="toc-text">在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E5%90%8E%EF%BC%8C%E7%BD%91%E7%BB%9C%E5%9F%BA%E4%BA%8E%E5%BD%93%E5%89%8D%E7%9A%84%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%B9%B6%E6%89%A7%E8%A1%8C%E4%B8%80%E4%B8%AA%E5%8A%A8%E4%BD%9C%EF%BC%8C%E4%BB%A5%E6%9C%9D%E5%90%91%E8%AF%A5%E8%88%AA%E8%B7%AF%E7%82%B9%E8%BF%90%E5%8A%A8%E3%80%82"><span class="toc-number">1.0.3.6.</span> <span class="toc-text">随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%B2%BF%E7%9D%80%E5%A4%9A%E4%B8%AA%E8%88%AA%E8%B7%AF%E7%82%B9%E9%80%90%E6%AD%A5%E5%90%91%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E7%A7%BB%E5%8A%A8%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%EF%BC%8C%E5%90%8C%E6%97%B6%E5%AE%8C%E6%88%90%E5%AF%B9%E7%8E%AF%E5%A2%83%E7%9A%84%E5%9C%B0%E5%9B%BE%E6%9E%84%E5%BB%BA%EF%BC%88%E5%BB%BA%E5%9B%BE%EF%BC%89%E3%80%82"><span class="toc-number">1.0.3.7.</span> <span class="toc-text">在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E5%85%A8%E5%B1%80%E5%AF%BC%E8%88%AA"><span class="toc-number">1.0.4.</span> <span class="toc-text">A.全局导航</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BA%86%E4%BD%BF%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%83%BD%E5%A4%9F%E6%9C%9D%E5%90%91%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E5%AF%BC%E8%88%AA%EF%BC%8C%E5%BF%85%E9%A1%BB%E4%BB%8E%E5%BD%93%E5%89%8D%E5%8F%AF%E7%94%A8%E7%9A%84%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%88POI%EF%BC%89%E4%B8%AD%E9%80%89%E6%8B%A9%E7%94%A8%E4%BA%8E%E5%B1%80%E9%83%A8%E5%AF%BC%E8%88%AA%E7%9A%84%E4%B8%AD%E9%97%B4%E8%88%AA%E8%B7%AF%E7%82%B9%E3%80%82"><span class="toc-number">1.0.4.1.</span> <span class="toc-text">为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B8%8D%E4%BB%85%E9%9C%80%E8%A6%81%E8%A2%AB%E5%BC%95%E5%AF%BC%E5%89%8D%E5%BE%80%E7%9B%AE%E6%A0%87%EF%BC%8C%E8%BF%98%E5%BF%85%E9%A1%BB%E5%9C%A8%E8%A1%8C%E8%BF%9B%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%8E%A2%E7%B4%A2%E5%91%A8%E5%9B%B4%E7%8E%AF%E5%A2%83%EF%BC%8C%E4%BB%A5%E4%BE%BF%E5%9C%A8%E9%81%87%E5%88%B0%E6%AD%BB%E8%B7%AF%E6%97%B6%E8%83%BD%E5%A4%9F%E8%AF%86%E5%88%AB%E5%87%BA%E5%8F%AF%E8%83%BD%E7%9A%84%E6%9B%BF%E4%BB%A3%E8%B7%AF%E5%BE%84%E3%80%82"><span class="toc-number">1.0.4.2.</span> <span class="toc-text">机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%89%B4%E4%BA%8E%E6%B2%A1%E6%9C%89%E9%A2%84%E5%85%88%E6%8F%90%E4%BE%9B%E7%9A%84%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF%EF%BC%8C%E6%89%80%E6%9C%89%E5%8F%AF%E8%83%BD%E7%9A%84-POI-%E5%BF%85%E9%A1%BB%E4%BB%8E%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BD%93%E5%89%8D%E7%9A%84%E5%91%A8%E8%BE%B9%E7%8E%AF%E5%A2%83%E4%B8%AD%E6%8F%90%E5%8F%96%EF%BC%8C%E5%B9%B6%E5%AD%98%E5%82%A8%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%AD%E4%BB%A5%E4%BE%9B%E5%90%8E%E7%BB%AD%E4%BD%BF%E7%94%A8%E3%80%82"><span class="toc-number">1.0.4.3.</span> <span class="toc-text">鉴于没有预先提供的环境信息，所有可能的 POI 必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E5%9C%A8%E5%90%8E%E7%BB%AD%E6%AD%A5%E9%AA%A4%E4%B8%AD%E5%8F%91%E7%8E%B0%E6%9F%90%E4%B8%AA%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%88POI%EF%BC%89%E4%BD%8D%E4%BA%8E%E9%9A%9C%E7%A2%8D%E7%89%A9%E9%99%84%E8%BF%91%EF%BC%8C%E5%88%99%E8%AF%A5%E7%82%B9%E4%BC%9A%E4%BB%8E%E5%86%85%E5%AD%98%E4%B8%AD%E5%88%A0%E9%99%A4%E3%80%82"><span class="toc-number">1.0.4.4.</span> <span class="toc-text">如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%B7%B2%E7%BB%8F%E8%AE%BF%E9%97%AE%E8%BF%87%E7%9A%84%E4%BD%8D%E7%BD%AE%EF%BC%8C%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E4%B8%8D%E4%BC%9A%E5%86%8D%E6%8F%90%E5%8F%96%E6%96%B0%E7%9A%84-POI%E3%80%82"><span class="toc-number">1.0.4.5.</span> <span class="toc-text">在机器人已经访问过的位置，激光雷达不会再提取新的 POI。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A4%E5%A4%96%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%9F%90%E4%B8%AA-POI-%E8%A2%AB%E9%80%89%E4%B8%BA%E8%88%AA%E8%B7%AF%E7%82%B9%E4%BD%86%E5%9C%A8%E8%8B%A5%E5%B9%B2%E6%AD%A5%E5%86%85%E5%A7%8B%E7%BB%88%E6%97%A0%E6%B3%95%E5%88%B0%E8%BE%BE%EF%BC%8C%E5%AE%83%E4%B9%9F%E4%BC%9A%E8%A2%AB%E5%88%A0%E9%99%A4%EF%BC%8C%E5%B9%B6%E9%87%8D%E6%96%B0%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84%E8%88%AA%E8%B7%AF%E7%82%B9%E3%80%82"><span class="toc-number">1.0.4.6.</span> <span class="toc-text">此外，如果某个 POI 被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%96%B0%E7%9A%84POI%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9A%E5%B0%B1%E6%98%AF%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84POI%E7%9A%84%E7%A1%AE%E5%AE%9A"><span class="toc-number">1.0.4.7.</span> <span class="toc-text">获取新的POI的方法：就是上面提到的POI的确定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E5%BD%93%E5%89%8D%E7%8E%AF%E5%A2%83%E4%B8%AD%E6%8F%90%E5%8F%96POI%EF%BC%8C%E8%93%9D%E8%89%B2%E5%9C%86%E5%9C%88%E8%A1%A8%E7%A4%BA%E9%80%9A%E8%BF%87%E7%9B%B8%E5%BA%94%E6%96%B9%E6%B3%95%E6%8F%90%E5%8F%96%E7%9A%84%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%8C%E7%BB%BF%E8%89%B2%E5%9C%86%E5%9C%88%E8%A1%A8%E7%A4%BA%E5%BD%93%E5%89%8D%E7%9A%84%E8%88%AA%E8%B7%AF%E7%82%B9%E3%80%82"><span class="toc-number">1.0.4.8.</span> <span class="toc-text">从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#a-%E8%93%9D%E8%89%B2-POI-%E6%98%AF%E9%80%9A%E8%BF%87%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E6%B5%8B%E8%B7%9D%E6%95%B0%E6%8D%AE%E4%B9%8B%E9%97%B4%E7%9A%84%E9%97%B4%E9%9A%99%E6%8F%90%E5%8F%96%E7%9A%84%EF%BC%9B"><span class="toc-number">1.0.4.9.</span> <span class="toc-text">(a) 蓝色 POI 是通过激光雷达测距数据之间的间隙提取的；</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#b-%E8%93%9D%E8%89%B2-POI-%E6%98%AF%E4%BB%8E%E9%9D%9E%E6%95%B0%E5%80%BC%E5%9E%8B%E7%9A%84%E6%BF%80%E5%85%89%E8%AF%BB%E6%95%B0%E4%B8%AD%E6%8F%90%E5%8F%96%E7%9A%84%E3%80%82"><span class="toc-number">1.0.4.10.</span> <span class="toc-text">(b)蓝色 POI 是从非数值型的激光读数中提取的。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E6%97%B6%E5%88%BB-t%EF%BC%8C%E4%BB%8E%E5%BD%93%E5%89%8D%E5%8F%AF%E7%94%A8%E7%9A%84%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%88POI%EF%BC%89%E4%B8%AD%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%9F%BA%E4%BA%8E%E4%BF%A1%E6%81%AF%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%8F%97%E9%99%90%E6%8E%A2%E7%B4%A2%E6%96%B9%E6%B3%95%EF%BC%88Information-based-Distance-Limited-Exploration%EF%BC%8C%E7%AE%80%E7%A7%B0-IDLE%EF%BC%89%E6%9D%A5%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E7%9A%84%E8%88%AA%E8%B7%AF%E7%82%B9%E3%80%82IDLE-%E6%96%B9%E6%B3%95%E9%80%9A%E8%BF%87%E4%BB%A5%E4%B8%8B%E6%96%B9%E5%BC%8F%E8%AF%84%E4%BC%B0%E6%AF%8F%E4%B8%AA%E5%80%99%E9%80%89-POI-%E7%9A%84%E9%80%82%E5%BA%94%E5%BA%A6%EF%BC%9A"><span class="toc-number">1.0.4.11.</span> <span class="toc-text">在时刻 t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based Distance Limited Exploration，简称 IDLE）来选择最优的航路点。IDLE 方法通过以下方式评估每个候选 POI 的适应度：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AF%8F%E4%B8%AA%E5%80%99%E9%80%89%E5%85%B4%E8%B6%A3%E7%82%B9-c-%E7%B4%A2%E5%BC%95%E4%B8%BA-i-%E7%9A%84%E5%BE%97%E5%88%86-h-%E7%94%B1%E4%B8%89%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90"><span class="toc-number">1.0.4.12.</span> <span class="toc-text">每个候选兴趣点 c (索引为 i) 的得分 h 由三部分组成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%B8%AD%EF%BC%8C%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%9C%A8%E6%97%B6%E5%88%BB-t-%E7%9A%84%E4%BD%8D%E7%BD%AE-p-t-%E4%B8%8E%E5%80%99%E9%80%89%E5%85%B4%E8%B6%A3%E7%82%B9-c-i-%E4%B9%8B%E9%97%B4%E7%9A%84%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB%E5%88%86%E9%87%8F-d-p-t-c-i-%E8%A2%AB%E8%A1%A8%E7%A4%BA%E4%B8%BA%E4%B8%80%E4%B8%AA%E5%8F%8C%E6%9B%B2%E6%AD%A3%E5%88%87%E5%87%BD%E6%95%B0%EF%BC%88tanh%EF%BC%89%E5%BD%A2%E5%BC%8F%EF%BC%9A"><span class="toc-number">1.0.4.13.</span> <span class="toc-text">其中，机器人在时刻 t 的位置 p_t 与候选兴趣点 c_i 之间的欧几里得距离分量 d(p_t, c_i) 被表示为一个双曲正切函数（tanh）形式：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%B8%AD%EF%BC%8Ce-%E6%98%AF%E8%87%AA%E7%84%B6%E5%AF%B9%E6%95%B0%E7%9A%84%E5%BA%95%EF%BC%88%E6%AC%A7%E6%8B%89%E6%95%B0%EF%BC%89%EF%BC%8Cl-1-%E5%92%8C-l-2-%E6%98%AF%E4%B8%A4%E4%B8%AA%E8%B7%9D%E7%A6%BB%E9%98%88%E5%80%BC%EF%BC%8C%E7%94%A8%E4%BA%8E%E5%AF%B9%E5%BE%97%E5%88%86%E8%BF%9B%E8%A1%8C%E8%A1%B0%E5%87%8F%E3%80%82%E8%BF%99%E4%B8%A4%E4%B8%AA%E8%B7%9D%E7%A6%BB%E9%98%88%E5%80%BC%E6%A0%B9%E6%8D%AE%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88DRL%EF%BC%89%E8%AE%AD%E7%BB%83%E7%8E%AF%E5%A2%83%E7%9A%84%E5%8C%BA%E5%9F%9F%E5%A4%A7%E5%B0%8F%E8%AE%BE%E5%AE%9A%E3%80%82-%E8%BF%99%E9%83%A8%E5%88%86%E6%98%AF%E4%B8%80%E4%B8%AA%E8%B7%9D%E7%A6%BB%E6%83%A9%E7%BD%9A%E9%A1%B9"><span class="toc-number">1.0.4.14.</span> <span class="toc-text">其中，e 是自然对数的底（欧拉数），l_1 和 l_2 是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。 这部分是一个距离惩罚项</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%EF%BC%9A"><span class="toc-number">1.0.4.15.</span> <span class="toc-text">注：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%AD%90%E4%B8%AD%E6%8C%87%E6%95%B0%E9%83%A8%E5%88%86%E7%9A%84-d-p-t-c-i-l-2-l-1-%E8%BF%99%E6%98%AF%E5%9C%A8%E5%AF%B9%E8%B7%9D%E7%A6%BB-d-%E5%81%9A%E5%BD%92%E4%B8%80%E5%8C%96%E5%A4%84%E7%90%86%EF%BC%8C%E6%8A%8A%E8%B7%9D%E7%A6%BB%E5%B0%BA%E5%BA%A6%E8%BD%AC%E5%8C%96%E4%B8%BA%E6%97%A0%E5%8D%95%E4%BD%8D%E7%9A%84%E6%AF%94%E5%80%BC%EF%BC%8Cl-2-l-1-%E6%98%AF%E8%B7%9D%E7%A6%BB%E7%AA%97%E5%8F%A3%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%9C%9F%E6%9C%9B-d-%E8%90%BD%E5%9C%A8%E8%BF%99%E4%B8%AA%E5%8C%BA%E9%97%B4%E9%87%8C%EF%BC%8C%E8%BF%99%E6%A0%B7%E5%81%9A%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E8%AE%A9%E8%B7%9D%E7%A6%BB-d-%E7%9A%84%E5%A4%A7%E5%B0%8F%E7%9B%B8%E5%AF%B9%E4%BA%8E%E2%80%9C%E5%85%81%E8%AE%B8%E7%9A%84%E8%B7%9D%E7%A6%BB%E8%8C%83%E5%9B%B4%E2%80%9D-%EF%BC%88l-2-l-1%EF%BC%89-%E6%9D%A5%E8%A1%A8%E8%BE%BE%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E7%BB%9D%E5%AF%B9%E8%B7%9D%E7%A6%BB%E5%A4%A7%E5%B0%8F%EF%BC%8C%E6%96%B9%E4%BE%BF%E5%9C%A8%E6%8C%87%E6%95%B0%E5%92%8C%E5%B9%B3%E6%96%B9%E8%AE%A1%E7%AE%97%E4%B8%AD%E4%BF%9D%E6%8C%81%E6%95%B0%E5%80%BC%E7%9A%84%E5%90%88%E7%90%86%E8%8C%83%E5%9B%B4%E3%80%82"><span class="toc-number">1.0.4.16.</span> <span class="toc-text">分子中指数部分的 d(p_t,c_i)&#x2F;(l_2 - l_1) 这是在对距离 d 做归一化处理，把距离尺度转化为无单位的比值，l_2 - l_1 是距离窗口，也就是期望 d 落在这个区间里，这样做的目的是让距离 d 的大小相对于“允许的距离范围” （l_2 - l_1） 来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#l-2-l-2-l-1-%E6%98%AF%E5%AF%B9-l-2-%E8%BF%9B%E8%A1%8C%E5%BD%92%E4%B8%80%E5%8C%96%E5%A4%84%E7%90%86%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%80%BC%E7%A1%AE%E5%AE%9A%E4%BA%86%E6%8C%87%E6%95%B0%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E2%80%9C%E6%9C%80%E5%A4%A7%E5%80%BC%E2%80%9D%E6%88%96%E8%80%85%E2%80%9C%E5%8F%82%E8%80%83%E5%80%BC%E2%80%9D%EF%BC%8C%E5%AE%83%E8%AE%A9%E6%8C%87%E6%95%B0%E4%B8%AD%E7%9A%84%E5%88%86%E5%AD%90%E5%92%8C%E5%88%86%E6%AF%8D%E5%9C%A8%E7%9B%B8%E5%90%8C%E7%9A%84%E5%B0%BA%E5%BA%A6%E4%B8%8B%E8%BF%9B%E8%A1%8C%E6%AF%94%E8%BE%83%EF%BC%8C%E5%AE%9E%E7%8E%B0%E5%AF%B9%E8%B7%9D%E7%A6%BB%E5%BE%97%E5%88%86%E7%9A%84%E5%90%88%E7%90%86%E7%BC%A9%E6%94%BE%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%82%E6%9C%AC%E8%B4%A8%E4%B8%8A%E4%B9%9F%E6%98%AF%E5%AE%9E%E7%8E%B0%E4%BA%86e-x-e-y-%E8%BF%99%E4%B8%AA%E5%88%86%E5%BC%8F%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96%E5%A4%84%E7%90%86%E3%80%82"><span class="toc-number">1.0.4.17.</span> <span class="toc-text">l_2 &#x2F; (l_2 - l_1) 是对 l_2 进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e^x &#x2F; e^y 这个分式的归一化处理。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E5%B9%B3%E6%96%B9%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%AE%A9%E8%B7%9D%E7%A6%BB%E5%B7%AE%E5%BC%82%E6%9B%B4%E6%95%8F%E6%84%9F%EF%BC%8C%E8%BF%9C%E7%A6%BB%E9%98%88%E5%80%BC%E7%9A%84%E7%82%B9%E6%9D%83%E9%87%8D%E5%8F%98%E5%8C%96%E6%9B%B4%E5%A4%A7%E3%80%82"><span class="toc-number">1.0.4.18.</span> <span class="toc-text">通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E6%8C%87%E6%95%B0%E6%94%BE%E5%A4%A7%E8%BF%99%E4%B8%AA%E5%8F%98%E5%8C%96%EF%BC%8C%E8%B7%9D%E7%A6%BB%E8%BE%83%E8%BF%9C%E6%88%96%E8%BE%83%E8%BF%91%E7%9A%84%E7%82%B9%E7%9A%84%E5%BE%97%E5%88%86%E8%BF%85%E9%80%9F%E5%87%8F%E5%B0%8F%E6%88%96%E5%A2%9E%E5%A4%A7%EF%BC%8C%E8%BE%BE%E5%88%B0%E5%AF%B9%E8%B7%9D%E7%A6%BB%E5%88%86%E9%87%8F%E7%9A%84%E5%B9%B3%E6%BB%91%E8%A1%B0%E5%87%8F%E6%95%88%E6%9E%9C%E3%80%82"><span class="toc-number">1.0.4.19.</span> <span class="toc-text">通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%99%E7%A7%8D%E5%B9%B3%E6%BB%91%E8%A1%B0%E5%87%8F%E9%81%BF%E5%85%8D%E4%BA%86%E8%B7%9D%E7%A6%BB%E7%9B%B4%E6%8E%A5%E7%BA%BF%E6%80%A7%E6%9D%83%E9%87%8D%E5%AF%BC%E8%87%B4%E7%9A%84%E6%9E%81%E7%AB%AF%E5%80%BC%E5%BD%B1%E5%93%8D%EF%BC%8C%E4%BD%BF%E5%BE%97%E5%88%86%E5%88%86%E5%B8%83%E6%9B%B4%E5%90%88%E7%90%86%E5%92%8C%E7%A8%B3%E5%AE%9A%E3%80%82"><span class="toc-number">1.0.4.20.</span> <span class="toc-text">这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%8F%8C%E6%9B%B2%E6%AD%A3%E5%88%87%E5%87%BD%E6%95%B0-tanh-%E5%B0%86%E6%8C%87%E6%95%B0%E7%BB%93%E6%9E%9C%E6%98%A0%E5%B0%84%E5%88%B0-0-1-%E4%B9%8B%E9%97%B4%EF%BC%8C%E8%B5%B7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%BC%A9%E6%94%BE%E5%92%8C%E5%B9%B3%E6%BB%91%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E4%BD%9C%E7%94%A8%E3%80%82"><span class="toc-number">1.0.4.21.</span> <span class="toc-text">使用双曲正切函数 tanh() 将指数结果映射到 (0, 1) 之间，起到非线性缩放和平滑归一化的作用。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E4%B9%98%E4%BB%A5-l-2-%E5%AF%B9%E8%B7%9D%E7%A6%BB%E5%88%86%E9%87%8F%E8%BF%9B%E8%A1%8C%E5%B0%BA%E5%BA%A6%E8%B0%83%E6%95%B4%EF%BC%8C%E6%98%A0%E5%B0%84%E5%88%B0-0-l-2-%E4%B9%8B%E9%97%B4"><span class="toc-number">1.0.4.22.</span> <span class="toc-text">最后乘以 l_2 对距离分量进行尺度调整，映射到 (0, l_2)之间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%99%E4%B8%80%E9%83%A8%E5%88%86%E5%BE%97%E5%88%B0%E7%9A%84%E5%80%BC%EF%BC%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8E%8B%E7%BC%A9%E8%BF%87%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BE%97%E5%88%86%E9%A1%B9%EF%BC%8C%E7%9B%AE%E7%9A%84%E5%B0%B1%E6%98%AF%E4%BD%BF%E8%B7%9D%E7%A6%BB%E5%A4%84%E4%BA%8E-l-1-l-2-%E4%B9%8B%E9%97%B4%E7%9A%84%E5%80%99%E9%80%89%E5%85%B4%E8%B6%A3%E7%82%B9%E7%9A%84%E5%BE%97%E5%88%86%E5%B9%B3%E6%BB%91%E4%B8%8A%E5%8D%87%EF%BC%8C%E6%9C%89%E5%88%A9%E4%BA%8E%E9%80%89%E7%82%B9%E7%AD%96%E7%95%A5%E6%9B%B4%E7%A8%B3%E5%AE%9A"><span class="toc-number">1.0.4.23.</span> <span class="toc-text">这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于 l_1~l_2 之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E4%B8%AA%E5%88%86%E9%87%8F-d-c-i-g-%E8%A1%A8%E7%A4%BA%E5%80%99%E9%80%89%E5%85%B4%E8%B6%A3%E7%82%B9-c-i-%E4%B8%8E%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E7%82%B9-g-%E4%B9%8B%E9%97%B4%E7%9A%84%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB%EF%BC%88%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E8%B7%9D%E7%A6%BB%EF%BC%89%E3%80%82"><span class="toc-number">1.0.4.24.</span> <span class="toc-text">第二个分量 d(c_i, g) 表示候选兴趣点 c_i 与全局目标点 g 之间的欧几里得距离（全局目标距离）。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%EF%BC%8C%E6%97%B6%E5%88%BB-t-%E7%9A%84%E5%9C%B0%E5%9B%BE%E4%BF%A1%E6%81%AF%E5%BE%97%E5%88%86%EF%BC%88%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E6%BF%80%E5%8A%B1%E9%A1%B9%EF%BC%89%E8%A1%A8%E7%A4%BA%E4%B8%BA%EF%BC%9A"><span class="toc-number">1.0.4.25.</span> <span class="toc-text">最后，时刻 t 的地图信息得分（信息增益激励项）表示为：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%B8%AD%EF%BC%8CI-i-t-%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="toc-number">1.0.4.26.</span> <span class="toc-text">其中，I_{i,t} 的计算方式如下：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%B8%AD%EF%BC%8Ck-%E8%A1%A8%E7%A4%BA%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E5%80%99%E9%80%89%E7%82%B9%E5%91%A8%E5%9B%B4%E4%BF%A1%E6%81%AF%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A4%A7%E5%B0%8F%EF%BC%8C%E5%80%99%E9%80%89%E7%82%B9%E7%9A%84%E5%9D%90%E6%A0%87%E4%B8%BA-x-%E5%92%8C-y-%EF%BC%8C%E8%80%8C-w-%E5%92%8C-h-%E5%88%86%E5%88%AB%E8%A1%A8%E7%A4%BA%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E5%AE%BD%E5%BA%A6%E5%92%8C%E9%AB%98%E5%BA%A6%E3%80%82"><span class="toc-number">1.0.4.27.</span> <span class="toc-text">其中，k 表示用于计算候选点周围信息的卷积核大小，候选点的坐标为 x 和 y ，而 w 和 h 分别表示卷积核的宽度和高度。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E5%85%AC%E5%BC%8F-1-%E4%B8%AD%EF%BC%8C%E5%85%B7%E6%9C%89%E6%9C%80%E5%B0%8F-IDLE-%E5%BE%97%E5%88%86%E7%9A%84%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%88POI%EF%BC%89%E8%A2%AB%E9%80%89%E4%B8%BA%E7%94%A8%E4%BA%8E%E5%B1%80%E9%83%A8%E5%AF%BC%E8%88%AA%E7%9A%84%E6%9C%80%E4%BC%98%E8%88%AA%E8%B7%AF%E7%82%B9%E3%80%82"><span class="toc-number">1.0.4.28.</span> <span class="toc-text">在公式 (1) 中，具有最小 IDLE 得分的兴趣点（POI）被选为用于局部导航的最优航路点。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%EF%BC%9A-1"><span class="toc-number">1.0.4.29.</span> <span class="toc-text">注：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#I-i-t-%E8%A1%A8%E7%A4%BA%E5%80%99%E9%80%89%E5%85%B4%E8%B6%A3%E7%82%B9%EF%BC%88POI%EF%BC%89c-i-%E5%9C%A8%E6%97%B6%E9%97%B4-t-%E7%9A%84%E4%BF%A1%E6%81%AF%E5%BE%97%E5%88%86%EF%BC%8C%E5%AE%83%E7%94%A8%E4%BA%8E%E8%A1%A1%E9%87%8F%E6%AD%A4%E7%82%B9%E5%91%A8%E5%9B%B4%E7%8E%AF%E5%A2%83%E7%9A%84%E2%80%9C%E4%BF%A1%E6%81%AF%E4%B8%B0%E5%AF%8C%E5%BA%A6%E2%80%9D%E6%88%96%E2%80%9C%E6%8E%A2%E7%B4%A2%E6%BD%9C%E5%8A%9B%E2%80%9D%EF%BC%8C%E5%BE%97%E5%88%86%E8%B6%8A%E9%AB%98%EF%BC%8C%E8%AF%B4%E6%98%8E%E6%AD%A4-POI-%E9%99%84%E8%BF%91%E8%BF%98%E6%9C%89%E6%9C%AA%E6%8E%A2%E7%B4%A2%E3%80%81%E6%9C%AA%E7%9F%A5%E6%88%96%E5%80%BC%E5%BE%97%E6%8E%A2%E7%B4%A2%E7%9A%84%E5%8C%BA%E5%9F%9F%E3%80%82"><span class="toc-number">1.0.4.30.</span> <span class="toc-text">I_{i,t} 表示候选兴趣点（POI）c_i 在时间 t 的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此 POI 附近还有未探索、未知或值得探索的区域。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C-x-y-%E6%98%AF%E5%9C%B0%E5%9B%BE%E4%B8%8A-x-y-%E7%82%B9%E7%9A%84%E7%BD%AE%E4%BF%A1%E5%80%BC%E6%88%96%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E5%80%BC%EF%BC%8C%E5%8F%AF%E8%83%BD%E6%9D%A5%E6%BA%90%E4%BA%8E%E5%8D%A0%E6%8D%AE%E6%A0%85%E6%A0%BC%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E7%86%B5%E5%80%BC%E3%80%81%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E3%80%81%E6%9C%AA%E7%9F%A5%E5%8C%BA%E5%9F%9F%E6%A0%87%E8%AE%B0%E3%80%81%E7%BD%AE%E4%BF%A1%E6%A6%82%E7%8E%87%E7%AD%89%EF%BC%8C%E9%80%9A%E5%B8%B8%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%80%BC%E8%B6%8A%E9%AB%98%EF%BC%8C%E8%A1%A8%E7%A4%BA%E8%AF%A5%E4%BD%8D%E7%BD%AE%E7%9A%84%E4%BF%A1%E6%81%AF%E8%B6%8A%E5%B0%91%E3%80%81%E8%B6%8A%E5%80%BC%E5%BE%97%E6%8E%A2%E7%B4%A2%E3%80%82"><span class="toc-number">1.0.4.31.</span> <span class="toc-text">C(x, y) 是地图上 (x, y) 点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E9%87%8D%E6%B1%82%E5%92%8C%EF%BC%9A%E5%AE%83%E6%98%AF%E5%9C%A8%E7%82%B9-x-y-%E7%9A%84%E5%91%A8%E5%9B%B4%E5%8F%96%E4%BA%86%E4%B8%80%E4%B8%AA%E5%A4%A7%E5%B0%8F%E4%B8%BA-k-k-%E7%9A%84%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%EF%BC%8C%E7%84%B6%E5%90%8E%E5%AF%B9%E8%AF%A5%E7%AA%97%E5%8F%A3%E5%86%85%E6%89%80%E6%9C%89%E4%BD%8D%E7%BD%AE%E7%9A%84-C-%E5%80%BC%E8%BF%9B%E8%A1%8C%E6%B1%82%E5%92%8C%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%9C%A8%E4%BB%A5%E5%80%99%E9%80%89%E7%82%B9%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E9%82%BB%E5%9F%9F%E4%B8%AD%EF%BC%8C%E7%BB%9F%E8%AE%A1%E8%AF%A5%E9%82%BB%E5%9F%9F%E2%80%9C%E4%BF%A1%E6%81%AF%E9%87%8F%E2%80%9D%E3%80%82"><span class="toc-number">1.0.4.32.</span> <span class="toc-text">双重求和：它是在点 (x, y) 的周围取了一个大小为 k * k 的滑动窗口，然后对该窗口内所有位置的 C 值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%A4%E4%BB%A5-k-2-%E6%98%AF%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%9D%87%E5%80%BC%E6%93%8D%E4%BD%9C%EF%BC%8C%E5%8D%B3%E6%8A%8A%E7%AA%97%E5%8F%A3%E5%86%85%E7%9A%84%E4%BF%A1%E6%81%AF%E6%80%BB%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E4%B8%BA%E5%B9%B3%E5%9D%87%E4%BF%A1%E6%81%AF%E5%80%BC%EF%BC%8C%E5%BE%97%E5%88%B0%E7%9A%84%E5%80%BC%E8%90%BD%E5%9C%A8-0-1-%E5%86%85%EF%BC%8C%E4%BE%BF%E4%BA%8E%E6%8C%87%E6%95%B0%E5%87%BD%E6%95%B0%E5%A4%84%E7%90%86%E3%80%82"><span class="toc-number">1.0.4.33.</span> <span class="toc-text">除以 k^2 是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在 [0, 1]内，便于指数函数处理。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E7%9B%AE%E7%9A%84%E5%B0%B1%E6%98%AF%E9%BC%93%E5%8A%B1%E6%8E%A2%E7%B4%A2%E6%9C%AA%E7%9F%A5%E5%8C%BA%E5%9F%9F"><span class="toc-number">1.0.4.34.</span> <span class="toc-text">其目的就是鼓励探索未知区域</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%88%B0%EF%BC%8Ch-%E7%9A%84%E4%B8%89%E4%B8%AA%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86%E5%A5%BD%E5%83%8F%E4%B8%8D%E8%83%BD%E7%9B%B4%E6%8E%A5%E7%9B%B8%E5%8A%A0%EF%BC%8C%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%BE%97%E5%88%B0%E7%9A%84%E6%98%AF%E4%B8%80%E4%B8%AA%E5%BE%97%E5%88%86%E5%80%BC%EF%BC%8C%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E6%98%AF%E4%B8%80%E4%B8%AA%E8%B7%9D%E7%A6%BB%E5%80%BC%EF%BC%8C%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%98%AF%E4%B8%80%E4%B8%AA%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E6%BF%80%E5%8A%B1%E5%80%BC%EF%BC%8C%E8%B2%8C%E4%BC%BC%E6%98%AF%E4%B8%8D%E8%83%BD%E7%9B%B4%E6%8E%A5%E7%9B%B8%E5%8A%A0%E7%9A%84%EF%BC%8C%E4%BD%86%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E8%BF%9B%E8%A1%8C%E4%BA%86%E4%B8%80%E4%B8%AA-l-2-%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E4%BC%AA%E8%B7%9D%E7%A6%BB%EF%BC%8C%E6%9C%80%E5%90%8E%E4%B8%80%E9%83%A8%E5%88%86%E4%B9%9F%E8%BF%9B%E8%A1%8C%E4%BA%86%E6%8C%87%E6%95%B0%E5%8C%96%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%89%80%E4%BB%A5%E6%98%AF%E5%8F%AF%E4%BB%A5%E7%9B%B8%E5%8A%A0%E7%9A%84"><span class="toc-number">1.0.4.35.</span> <span class="toc-text">注意到，h 的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个 *l_2 的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E5%B1%80%E9%83%A8%E5%AF%BC%E8%88%AA"><span class="toc-number">1.0.5.</span> <span class="toc-text">B.局部导航</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9F%BA%E4%BA%8E-%E5%8F%8C%E5%BB%B6%E8%BF%9F%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-TD3%EF%BC%8C%E4%B8%80%E7%A7%8D-Actor-Critic-%E6%9E%B6%E6%9E%84-%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E6%9D%A5%E8%AE%AD%E7%BB%83%E8%BF%90%E5%8A%A8%E7%AD%96%E7%95%A5"><span class="toc-number">1.0.5.1.</span> <span class="toc-text">使用基于 双延迟深度确定性策略梯度(TD3，一种 Actor-Critic 架构)的神经网络架构来训练运动策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF%E5%92%8C%E7%9B%AE%E6%A0%87%E8%88%AA%E7%82%B9%E7%9B%B8%E5%AF%B9%E4%BA%8E-agent-%E4%BD%8D%E7%BD%AE%E7%9A%84%E6%9E%81%E5%9D%90%E6%A0%87%E4%B8%80%E8%B5%B7%EF%BC%8C%E4%BD%9C%E4%B8%BA%E7%8A%B6%E6%80%81%E8%BE%93%E5%85%A5-s-%E4%BC%A0%E5%85%A5-TD3-%E7%9A%84Actor-%E7%BD%91%E7%BB%9C%E4%B8%AD"><span class="toc-number">1.0.5.2.</span> <span class="toc-text">局部环境信息和目标航点相对于 agent 位置的极坐标一起，作为状态输入 s 传入 TD3 的Actor 网络中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%A5-Actor-%E7%BD%91%E7%BB%9C%E7%94%B1%E4%B8%A4%E4%B8%AA%E5%85%A8%E8%BF%9E%E6%8E%A5%EF%BC%88FC%EF%BC%89%E5%B1%82%E7%BB%84%E6%88%90%EF%BC%8C%E6%AF%8F%E4%B8%80%E5%B1%82%E5%90%8E%E9%9D%A2%E9%83%BD%E6%8E%A5%E6%9C%89-ReLU%EF%BC%88%E4%BF%AE%E6%AD%A3%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%89%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E3%80%82"><span class="toc-number">1.0.5.3.</span> <span class="toc-text">该 Actor 网络由两个全连接（FC）层组成，每一层后面都接有 ReLU（修正线性单元）激活函数。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E4%B8%80%E5%B1%82%E4%B8%8E%E8%BE%93%E5%87%BA%E5%B1%82%E7%9B%B8%E8%BF%9E%EF%BC%8C%E8%BE%93%E5%87%BA%E4%B8%A4%E4%B8%AA%E5%8A%A8%E4%BD%9C%E5%8F%82%E6%95%B0-a%EF%BC%8C%E5%88%86%E5%88%AB%E8%A1%A8%E7%A4%BA%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E7%BA%BF%E9%80%9F%E5%BA%A6-a-1-%E5%92%8C%E8%A7%92%E9%80%9F%E5%BA%A6-a-2"><span class="toc-number">1.0.5.4.</span> <span class="toc-text">最后一层与输出层相连，输出两个动作参数 a，分别表示机器人的线速度 a_1 和角速度 a_2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%E9%87%87%E7%94%A8-tanh-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%8C%E5%B0%86%E8%BE%93%E5%87%BA%E9%99%90%E5%88%B6%E5%9C%A8%E5%8C%BA%E9%97%B4-%E2%88%921-1-%E5%86%85"><span class="toc-number">1.0.5.5.</span> <span class="toc-text">输出层采用 tanh 激活函数，将输出限制在区间 (−1,1) 内</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E5%B0%86%E5%8A%A8%E4%BD%9C%E5%BA%94%E7%94%A8%E4%BA%8E%E7%8E%AF%E5%A2%83%E4%B9%8B%E5%89%8D%EF%BC%8C%E5%AE%83%E4%BB%AC%E4%BC%9A%E6%8C%89%E4%BB%A5%E4%B8%8B%E6%96%B9%E5%BC%8F%E7%BC%A9%E6%94%BE%E4%B8%BA%E5%AE%9E%E9%99%85%E9%80%9F%E5%BA%A6%E5%80%BC%EF%BC%9A"><span class="toc-number">1.0.5.6.</span> <span class="toc-text">在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E7%BA%BF%E9%80%9F%E5%BA%A6-v-max%EF%BC%8C%E6%9C%80%E5%A4%A7%E8%A7%92%E9%80%9F%E5%BA%A6-%CF%89-max"><span class="toc-number">1.0.5.7.</span> <span class="toc-text">最大线速度 v_max，最大角速度 ω_max</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%B1%E4%BA%8E%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E5%8F%AA%E8%AE%B0%E5%BD%95%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%89%8D%E6%96%B9%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%9B%A0%E6%AD%A4%E4%B8%8D%E8%80%83%E8%99%91%E5%90%91%E5%90%8E%E7%9A%84%E8%BF%90%E5%8A%A8%EF%BC%8C%E5%B9%B6%E5%B0%86%E7%BA%BF%E9%80%9F%E5%BA%A6%E8%B0%83%E6%95%B4%E4%B8%BA%E4%BB%85%E4%B8%BA%E6%AD%A3%E5%80%BC%E3%80%82"><span class="toc-number">1.0.5.8.</span> <span class="toc-text">由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8A%B6%E6%80%81-%E5%8A%A8%E4%BD%9C%E5%AF%B9%E7%9A%84-Q-%E5%80%BC-Q-s-a-%E7%94%B1%E4%B8%A4%E4%B8%AA-Critic-%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E8%AF%84%E4%BC%B0%E3%80%82%E8%BF%99%E4%B8%A4%E4%B8%AA-Critic-%E7%BD%91%E7%BB%9C%E5%85%B7%E6%9C%89%E7%9B%B8%E5%90%8C%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%8C%E4%BD%86%E5%85%B6%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E6%98%AF%E5%BB%B6%E8%BF%9F%E8%BF%9B%E8%A1%8C%E7%9A%84%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%85%81%E8%AE%B8%E5%AE%83%E4%BB%AC%E5%9C%A8%E5%8F%82%E6%95%B0%E4%B8%8A%E4%BA%A7%E7%94%9F%E5%B7%AE%E5%BC%82%EF%BC%88%E9%81%BF%E5%85%8D%E5%AE%8C%E5%85%A8%E5%90%8C%E6%AD%A5%EF%BC%89%E3%80%82"><span class="toc-number">1.0.5.9.</span> <span class="toc-text">状态-动作对的 Q 值 Q(s,a) 由两个 Critic 网络进行评估。这两个 Critic 网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Critic-%E7%BD%91%E7%BB%9C%E4%BB%A5%E7%8A%B6%E6%80%81-s-%E5%92%8C%E5%8A%A8%E4%BD%9C-a-%E7%9A%84%E7%BB%84%E5%90%88%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%85%A5"><span class="toc-number">1.0.5.10.</span> <span class="toc-text">Critic 网络以状态 s 和动作 a 的组合作为输入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%B8%AD%EF%BC%8C%E7%8A%B6%E6%80%81-s-%E9%A6%96%E5%85%88%E8%A2%AB%E9%80%81%E5%85%A5%E4%B8%80%E4%B8%AA%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%8C%E5%B9%B6%E6%8E%A5%E4%B8%8A%E4%B8%80%E4%B8%AA-ReLU-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%8C%E8%BE%93%E5%87%BA%E4%B8%BA-L-s"><span class="toc-number">1.0.5.11.</span> <span class="toc-text">其中，状态 s 首先被送入一个全连接层，并接上一个 ReLU 激活函数，输出为 L_s</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%A5%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA-L-s-%E4%BB%A5%E5%8F%8A%E5%8A%A8%E4%BD%9C-a%EF%BC%8C%E5%B0%86%E5%88%86%E5%88%AB%E9%80%81%E5%85%A5%E4%B8%A4%E4%B8%AA%E5%A4%A7%E5%B0%8F%E7%9B%B8%E5%90%8C%E7%9A%84%E5%8F%98%E6%8D%A2%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88TFC%EF%BC%89%E4%B8%AD%EF%BC%8C%E5%88%86%E5%88%AB%E5%AF%B9%E5%BA%94%E5%8F%98%E6%8D%A2%E7%BB%93%E6%9E%9C%E4%B8%BA-%CF%841-%E5%92%8C-%CF%842%EF%BC%8C%E9%9A%8F%E5%90%8E%EF%BC%8C%E8%BF%99%E4%B8%A4%E4%B8%AA%E7%BB%93%E6%9E%9C%E6%8C%89%E5%A6%82%E4%B8%8B%E6%96%B9%E5%BC%8F%E8%BF%9B%E8%A1%8C%E7%BB%84%E5%90%88%EF%BC%9A"><span class="toc-number">1.0.5.12.</span> <span class="toc-text">该层的输出 L_s 以及动作 a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为 τ1 和 τ2，随后，这两个结果按如下方式进行组合：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%B8%AD%EF%BC%8CL-c-%E6%98%AF%E7%BB%84%E5%90%88%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88CFC%EF%BC%89%E7%9A%84%E8%BE%93%E5%87%BA%EF%BC%8CW-%CF%84-1-%E5%92%8C-W-%CF%84-2-%E5%88%86%E5%88%AB%E6%98%AF-%CF%841-%E5%92%8C-%CF%842-%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%8Cb-%CF%84-2-%E6%98%AF-%CF%84-2-%E7%9A%84%E5%81%8F%E6%89%A7%E9%A1%B9%EF%BC%8C%E7%84%B6%E5%90%8E%E5%9C%A8%E8%BF%99%E4%B8%AA%E7%BB%84%E5%90%88%E5%B1%82%E4%B8%8A%E4%BD%BF%E7%94%A8ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8E%A5%E7%9D%80%EF%BC%8C%E8%AF%A5%E8%BE%93%E5%87%BA%E8%BF%9E%E6%8E%A5%E5%88%B0%E4%B8%80%E4%B8%AA%E6%9C%80%E7%BB%88%E8%BE%93%E5%87%BA%E8%8A%82%E7%82%B9%EF%BC%8C%E8%AF%A5%E8%8A%82%E7%82%B9%E5%8C%85%E5%90%AB%E4%B8%80%E4%B8%AA%E5%8F%82%E6%95%B0%EF%BC%8C%E8%A1%A8%E7%A4%BA%E5%AF%B9%E5%BA%94%E7%8A%B6%E6%80%81-%E5%8A%A8%E4%BD%9C%E5%AF%B9%E7%9A%84-Q-%E5%80%BC%E3%80%82%E6%9C%80%E7%BB%88%EF%BC%8C%E4%BB%8E%E4%B8%A4%E4%B8%AA-Critic-%E7%BD%91%E7%BB%9C%E4%B8%AD%E9%80%89%E6%8B%A9%E8%BE%83%E5%B0%8F%E7%9A%84-Q-%E5%80%BC%EF%BC%8C%E4%BD%9C%E4%B8%BA%E6%9C%80%E5%90%8E%E7%9A%84-Critic-%E8%BE%93%E5%87%BA%EF%BC%8C%E4%BB%A5%E6%AD%A4%E6%9D%A5%E9%99%90%E5%88%B6%E5%AF%B9%E7%8A%B6%E6%80%81-%E5%8A%A8%E4%BD%9C%E5%80%BC%E7%9A%84%E8%BF%87%E9%AB%98%E4%BC%B0%E8%AE%A1%E3%80%82"><span class="toc-number">1.0.5.13.</span> <span class="toc-text">其中，L_c 是组合全连接层（CFC）的输出，W_τ_1 和 W_τ_2 分别是 τ1 和 τ2 的权重，b_τ_2 是 τ_2 的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的 Q 值。最终，从两个 Critic 网络中选择较小的 Q 值，作为最后的 Critic 输出，以此来限制对状态-动作值的过高估计。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E7%9A%84%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%A6%82%E5%9B%BE"><span class="toc-number">1.0.5.14.</span> <span class="toc-text">完整的网络架构如图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TD3-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%8C%85%E6%8B%AC-actor-%E5%92%8C-critic-%E4%B8%A4%E4%B8%AA%E9%83%A8%E5%88%86%E3%80%82%E6%AF%8F%E4%B8%80%E5%B1%82%E7%9A%84%E7%B1%BB%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%AF%B9%E5%BA%94%E7%9A%84%E5%8F%82%E6%95%B0%E6%95%B0%E9%87%8F%E5%9C%A8%E5%B1%82%E4%B8%AD%E9%83%BD%E6%9C%89%E6%8F%8F%E8%BF%B0%E3%80%82TFC-%E5%B1%82%E6%8C%87%E7%9A%84%E6%98%AF%E5%8F%98%E6%8D%A2%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82-%CF%84%EF%BC%8CCFC-%E5%B1%82%E6%8C%87%E7%9A%84%E6%98%AF%E7%BB%84%E5%90%88%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82-Lc%E3%80%82"><span class="toc-number">1.0.5.15.</span> <span class="toc-text">TD3 网络结构包括 actor 和 critic 两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC 层指的是变换全连接层 τ，CFC 层指的是组合全连接层 Lc。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E7%9A%84%E5%A5%96%E5%8A%B1%E4%BE%9D%E6%8D%AE%E4%BB%A5%E4%B8%8B%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AF%84%E4%BC%B0-%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.0.5.16.</span> <span class="toc-text">策略的奖励依据以下函数进行评估(奖励函数)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E6%97%B6%E9%97%B4%E6%AD%A5-t%E6%97%B6%EF%BC%8C%E7%8A%B6%E6%80%81-%E5%8A%A8%E4%BD%9C%E5%AF%B9-s-t-a-t-%E7%9A%84%E5%A5%96%E5%8A%B1-r-%E5%8F%96%E5%86%B3%E4%BA%8E%E4%BB%A5%E4%B8%8B%E4%B8%89%E7%A7%8D%E6%83%85%E5%86%B5%EF%BC%9A"><span class="toc-number">1.0.5.17.</span> <span class="toc-text">在时间步 t时，状态-动作对 (s_t, a_t) 的奖励 r 取决于以下三种情况：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E5%BD%93%E5%89%8D%E6%97%B6%E9%97%B4%E6%AD%A5%E4%B8%8E%E7%9B%AE%E6%A0%87%E7%82%B9%E7%9A%84%E8%B7%9D%E7%A6%BB-D-t-%E5%B0%8F%E4%BA%8E%E9%98%88%E5%80%BC-%CE%B7-D%EF%BC%8C%E5%88%99%E7%BB%99%E4%BA%88%E4%B8%80%E4%B8%AA%E6%AD%A3%E7%9A%84%E7%9B%AE%E6%A0%87%E5%A5%96%E5%8A%B1-r-g%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E9%BC%93%E5%8A%B1%EF%BC%89"><span class="toc-number">1.0.5.18.</span> <span class="toc-text">如果当前时间步与目标点的距离 D_t 小于阈值 η_D，则给予一个正的目标奖励 r_g（也就是鼓励）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E6%A3%80%E6%B5%8B%E5%88%B0%E7%A2%B0%E6%92%9E%EF%BC%8C%E5%88%99%E7%BB%99%E4%BA%88%E4%B8%80%E4%B8%AA%E8%B4%9F%E7%9A%84%E7%A2%B0%E6%92%9E%E6%83%A9%E7%BD%9A-r-c%EF%BC%88%E6%83%A9%E7%BD%9A%EF%BC%89"><span class="toc-number">1.0.5.19.</span> <span class="toc-text">如果检测到碰撞，则给予一个负的碰撞惩罚 r_c（惩罚）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E4%BB%A5%E4%B8%8A%E4%B8%A4%E7%A7%8D%E6%83%85%E5%86%B5%E5%9D%87%E6%9C%AA%E5%8F%91%E7%94%9F%EF%BC%8C%E5%88%99%E6%A0%B9%E6%8D%AE%E5%BD%93%E5%89%8D%E7%9A%84%E7%BA%BF%E9%80%9F%E5%BA%A6-v-%E5%92%8C%E8%A7%92%E9%80%9F%E5%BA%A6-%CF%89-%E7%BB%99%E4%BA%88%E5%8D%B3%E6%97%B6%E5%A5%96%E5%8A%B1%E3%80%82"><span class="toc-number">1.0.5.20.</span> <span class="toc-text">如果以上两种情况均未发生，则根据当前的线速度 v 和角速度 ω 给予即时奖励。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BA%86%E5%BC%95%E5%AF%BC%E5%AF%BC%E8%88%AA%E7%AD%96%E7%95%A5%E6%9C%9D%E5%90%91%E7%BB%99%E5%AE%9A%E7%9B%AE%E6%A0%87%EF%BC%8C%E4%B8%80%E4%B8%AA%E5%BB%B6%E8%BF%9F%E5%BD%92%E5%9B%A0%E5%A5%96%E5%8A%B1%E6%96%B9%E6%B3%95%E8%A2%AB%E9%87%87%E7%94%A8%EF%BC%8C%E5%85%B6%E8%AE%A1%E7%AE%97%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="toc-number">1.0.5.21.</span> <span class="toc-text">为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%B8%AD%EF%BC%8Cn-%E8%A1%A8%E7%A4%BA%E5%89%8D-n-%E4%B8%AA%E6%97%B6%E9%97%B4%E6%AD%A5%E4%B8%AD%E9%9C%80%E8%A6%81%E6%9B%B4%E6%96%B0%E5%A5%96%E5%8A%B1%E7%9A%84%E6%AD%A5%E6%95%B0%E3%80%82%E8%BF%99%E6%84%8F%E5%91%B3%E7%9D%80%EF%BC%8C%E6%AD%A3%E7%9A%84%E7%9B%AE%E6%A0%87%E5%A5%96%E5%8A%B1%E4%B8%8D%E4%BB%85%E8%A2%AB%E5%88%86%E9%85%8D%E7%BB%99%E8%BE%BE%E5%88%B0%E7%9B%AE%E6%A0%87%E7%9A%84%E9%82%A3%E4%B8%AA%E7%8A%B6%E6%80%81-%E5%8A%A8%E4%BD%9C%E5%AF%B9%EF%BC%8C%E8%BF%98%E4%BB%A5%E9%80%92%E5%87%8F%E7%9A%84%E6%96%B9%E5%BC%8F%E5%88%86%E9%85%8D%E7%BB%99%E5%85%B6%E4%B9%8B%E5%89%8D%E7%9A%84-n-%E4%B8%AA%E6%97%B6%E9%97%B4%E6%AD%A5%E3%80%82%E9%80%9A%E8%BF%87%E8%BF%99%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%8C%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E5%88%B0%E4%BA%86%E4%B8%80%E7%A7%8D%E5%B1%80%E9%83%A8%E5%AF%BC%E8%88%AA%E7%AD%96%E7%95%A5%EF%BC%8C%E8%AF%A5%E7%AD%96%E7%95%A5%E8%83%BD%E5%A4%9F%E4%BB%85%E4%BE%9D%E8%B5%96%E6%BF%80%E5%85%89%E8%BE%93%E5%85%A5%EF%BC%8C%E9%81%BF%E5%BC%80%E9%9A%9C%E7%A2%8D%E7%89%A9%E7%9A%84%E5%90%8C%E6%97%B6%E5%88%B0%E8%BE%BE%E5%B1%80%E9%83%A8%E7%9B%AE%E6%A0%87%E3%80%82"><span class="toc-number">1.0.5.22.</span> <span class="toc-text">其中，n 表示前 n 个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的 n 个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%BB%BA%E5%9B%BE"><span class="toc-number">1.0.6.</span> <span class="toc-text">C.探索与建图</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%B2%BF%E7%9D%80%E8%B7%AF%E5%BE%84%E7%82%B9%E8%A2%AB%E5%BC%95%E5%AF%BC%E5%90%91%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%E5%89%8D%E8%BF%9B%E3%80%82%E4%B8%80%E6%97%A6%E9%9D%A0%E8%BF%91%E5%85%A8%E5%B1%80%E7%9B%AE%E6%A0%87%EF%BC%8C%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%B0%86%E8%87%AA%E4%B8%BB%E5%AF%BC%E8%88%AA%E8%87%B3%E8%AF%A5%E7%9B%AE%E6%A0%87%E3%80%82%E5%9C%A8%E6%AD%A4%E8%BF%87%E7%A8%8B%E4%B8%AD%EF%BC%8C%E7%8E%AF%E5%A2%83%E8%A2%AB%E9%80%90%E6%AD%A5%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%BB%BA%E5%9B%BE%E3%80%82%E5%BB%BA%E5%9B%BE%E4%BE%9D%E8%B5%96%E4%BA%8E%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E5%92%8C%E6%9C%BA%E5%99%A8%E4%BA%BA%E9%87%8C%E7%A8%8B%E8%AE%A1%E4%BC%A0%E6%84%9F%E5%99%A8%E6%95%B0%E6%8D%AE%EF%BC%8C%E7%94%9F%E6%88%90%E7%8E%AF%E5%A2%83%E7%9A%84%E5%8D%A0%E6%8D%AE%E6%A0%85%E6%A0%BC%E5%9C%B0%E5%9B%BE%E3%80%82%E5%AE%8C%E6%95%B4%E7%9A%84%E8%87%AA%E4%B8%BB%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%BB%BA%E5%9B%BE%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%AA%E4%BB%A3%E7%A0%81%E5%9C%A8%E7%AE%97%E6%B3%951%E4%B8%AD%E8%BF%9B%E8%A1%8C%E4%BA%86%E6%8F%8F%E8%BF%B0%E3%80%82"><span class="toc-number">1.0.6.1.</span> <span class="toc-text">机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E9%83%A8%E5%88%86"><span class="toc-number">1.0.7.</span> <span class="toc-text">实验部分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E8%BF%87%E7%A8%8B%EF%BC%9A"><span class="toc-number">1.0.7.1.</span> <span class="toc-text">算法过程：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-number">1.0.7.2.</span> <span class="toc-text"></span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.0.8.</span> <span class="toc-text">A.系统设置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E4%BD%9C%E8%80%85%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%EF%BC%9A"><span class="toc-number">1.0.8.1.</span> <span class="toc-text">原作者系统配置：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%98%BE%E5%8D%A1%EF%BC%9ANVIDIA-GTX-1080"><span class="toc-number">1.0.8.2.</span> <span class="toc-text">显卡：NVIDIA GTX 1080</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E5%86%85%E5%AD%98%EF%BC%9A32G"><span class="toc-number">1.0.8.3.</span> <span class="toc-text">运行内存：32G</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CPU%EF%BC%9A-Intel-Core-i7-6800K"><span class="toc-number">1.0.8.4.</span> <span class="toc-text">CPU： Intel Core i7-6800K</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.0.8.5.</span> <span class="toc-text">训练参数设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TD3%E7%BD%91%E7%BB%9C%E5%9C%A8Gazebo%E4%B8%8A%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87ROS%E8%BF%9B%E8%A1%8C%E6%8E%A7%E5%88%B6%EF%BC%8C%E8%AE%AD%E7%BB%83%E4%BA%86800%E5%9B%9E%E5%90%88%EF%BC%8C%E7%BA%A68h"><span class="toc-number">1.0.8.6.</span> <span class="toc-text">TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AF%8F%E4%B8%AA%E8%AE%AD%E7%BB%83%E5%9B%9E%E5%90%88%E5%9C%A8%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%88%B0%E8%BE%BE%E7%9B%AE%E6%A0%87%E3%80%81%E5%8F%91%E7%94%9F%E7%A2%B0%E6%92%9E%E6%88%96%E6%89%A7%E8%A1%8C%E4%BA%86-500-%E6%AD%A5%E5%8A%A8%E4%BD%9C%E5%90%8E%E7%BB%93%E6%9D%9F"><span class="toc-number">1.0.8.7.</span> <span class="toc-text">每个训练回合在机器人到达目标、发生碰撞或执行了 500 步动作后结束</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E7%BA%BF%E9%80%9F%E5%BA%A6-v-max-%E5%92%8C%E6%9C%80%E5%A4%A7%E8%A7%92%E9%80%9F%E5%BA%A6-%CF%89-max-%E5%88%86%E5%88%AB%E8%AE%BE%E7%BD%AE%E4%B8%BA-0-5-m-s-%E5%92%8C-1rad-s"><span class="toc-number">1.0.8.8.</span> <span class="toc-text">最大线速度 v_max 和最大角速度 ω_max 分别设置为 0.5 m&#x2F;s 和 1rad&#x2F;s</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BB%B6%E8%BF%9F%E5%A5%96%E5%8A%B1%E5%9C%A8%E6%9C%80%E5%90%8E-n-10-%E6%AD%A5%E4%B8%AD%E6%9B%B4%E6%96%B0%EF%BC%8C%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E5%BB%B6%E8%BF%9F%E8%AE%BE%E7%BD%AE%E4%B8%BA%E6%AF%8F-2-%E4%B8%AA%E5%9B%9E%E5%90%88"><span class="toc-number">1.0.8.9.</span> <span class="toc-text">延迟奖励在最后 n&#x3D;10 步中更新，参数更新延迟设置为每 2 个回合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%9C%A8%E4%B8%80%E4%B8%AA-10x10-%E7%B1%B3%E7%9A%84%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83%E4%B8%AD%E8%BF%9B%E8%A1%8C%EF%BC%8C%E5%A6%82%E5%9B%BE%E6%89%80%E7%A4%BA"><span class="toc-number">1.0.8.10.</span> <span class="toc-text">训练在一个 10x10 米的模拟环境中进行，如图所示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%8E%AF%E5%A2%83%E7%A4%BA%E4%BE%8B%E3%80%82%E8%93%9D%E8%89%B2%E5%8C%BA%E5%9F%9F%E8%A1%A8%E7%A4%BA%E8%BE%93%E5%85%A5%E7%9A%84%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E8%AF%BB%E6%95%B0%E5%92%8C%E6%B5%8B%E8%B7%9D%E3%80%82%E5%9B%9B%E4%B8%AA%E7%AE%B1%E5%BD%A2%E9%9A%9C%E7%A2%8D%E7%89%A9%E5%9C%A8%E6%AF%8F%E4%B8%80%E8%BD%AE%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%83%BD%E4%BC%9A%E6%94%B9%E5%8F%98%E4%BD%8D%E7%BD%AE%EF%BC%8C%E5%A6%82%E5%9B%BE-a-%E3%80%81-b-%E5%92%8C-c-%E6%89%80%E7%A4%BA%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%8C%96%E3%80%82"><span class="toc-number">1.0.8.11.</span> <span class="toc-text">训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图(a)、(b)和(c)所示，以实现训练数据的随机化。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BF%83%E8%BF%9B%E7%AD%96%E7%95%A5%E7%9A%84%E6%B3%9B%E5%8C%96%E4%B8%8E%E6%8E%A2%E7%B4%A2%EF%BC%8C%E5%90%91%E4%BC%A0%E6%84%9F%E5%99%A8%E8%AF%BB%E6%95%B0%E5%92%8C%E5%8A%A8%E4%BD%9C%E5%80%BC%E4%B8%AD%E6%B7%BB%E5%8A%A0%E4%BA%86%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0%E3%80%82%E4%B8%BA%E4%BA%86%E6%9E%84%E5%BB%BA%E5%A4%9A%E6%A0%B7%E5%8C%96%E7%9A%84%E8%AE%AD%E7%BB%83%E7%8E%AF%E5%A2%83%EF%BC%8C%E5%9C%A8%E6%AF%8F%E4%B8%AA%E5%9B%9E%E5%90%88%E5%BC%80%E5%A7%8B%E6%97%B6%EF%BC%8C%E7%AE%B1%E5%BD%A2%E9%9A%9C%E7%A2%8D%E7%89%A9%E7%9A%84%E4%BD%8D%E7%BD%AE%E4%BC%9A%E8%A2%AB%E9%9A%8F%E6%9C%BA%E5%8C%96%E3%80%82%E5%85%B6%E4%BD%8D%E7%BD%AE%E5%8F%98%E5%8C%96%E7%9A%84%E7%A4%BA%E4%BE%8B%E5%A6%82%E5%9B%BE-a-%E3%80%81-b-%E3%80%81-c-%E6%89%80%E7%A4%BA%E3%80%82%E5%90%8C%E6%97%B6%EF%BC%8C%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%B5%B7%E5%A7%8B%E4%BD%8D%E7%BD%AE%E4%B8%8E%E7%9B%AE%E6%A0%87%E4%BD%8D%E7%BD%AE%E4%B9%9F%E5%9C%A8%E6%AF%8F%E4%B8%80%E5%9B%9E%E5%90%88%E4%B8%AD%E8%A2%AB%E9%9A%8F%E6%9C%BA%E8%AE%BE%E7%BD%AE%E3%80%82"><span class="toc-number">1.0.8.12.</span> <span class="toc-text">为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图 (a)、(b)、(c) 所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ROS-%E4%B8%AD%E7%9A%84-SLAM-Toolbox-%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%94%A8%E4%BA%8E%E7%94%9F%E6%88%90%E5%92%8C%E6%9B%B4%E6%96%B0%E7%8E%AF%E5%A2%83%E7%9A%84%E5%85%A8%E5%B1%80%E5%9C%B0%E5%9B%BE%EF%BC%8C%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%9C%A8%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%9A%84%E5%AE%9A%E4%BD%8D"><span class="toc-number">1.0.8.13.</span> <span class="toc-text">ROS 中的 SLAM Toolbox 软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ROS%E7%9A%84%E6%9C%AC%E5%9C%B0%E8%A7%84%E5%88%92%E5%99%A8%E5%8C%85%EF%BC%88TrajectoryPlanner%EF%BC%89%E4%BB%A3%E6%9B%BF%E4%BA%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.0.8.14.</span> <span class="toc-text">ROS的本地规划器包（TrajectoryPlanner）代替了神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E9%A9%B1%E5%8A%A8%E8%87%AA%E4%B8%BB%E6%8E%A2%E7%B4%A2%EF%BC%88GDAE-Goal-Driven-Autonomous-Exploration%EF%BC%89"><span class="toc-number">1.0.8.15.</span> <span class="toc-text">目标驱动自主探索（GDAE, Goal-Driven Autonomous Exploration）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E8%BF%91%E5%89%8D%E6%B2%BF%E6%8E%A2%E7%B4%A2%E7%AD%96%E7%95%A5%EF%BC%88Nearest-Frontier-NF%EF%BC%89"><span class="toc-number">1.0.8.16.</span> <span class="toc-text">最近前沿探索策略（Nearest Frontier, NF）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E9%A9%B1%E5%8A%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88GD-RL-Goal-Driven-Reinforcement-Learning%EF%BC%89"><span class="toc-number">1.0.8.17.</span> <span class="toc-text">目标驱动强化学习（GD-RL, Goal-Driven Reinforcement Learning）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E8%A7%84%E5%88%92%E5%99%A8%E8%87%AA%E4%B8%BB%E6%8E%A2%E7%B4%A2%EF%BC%88LP-AE%EF%BC%8CLocal-Planner-Autonomous-Exploration%EF%BC%89"><span class="toc-number">1.0.8.18.</span> <span class="toc-text">本地规划器自主探索（LP-AE，Local Planner Autonomous Exploration）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%E5%99%A8%EF%BC%88PP%EF%BC%8CPath-Planner%EF%BC%89%E5%AE%83%E6%98%AF%E5%9F%BA%E4%BA%8E-Dijkstra-%E7%9A%84%E7%94%9F%E6%88%90%E8%B7%AF%E5%BE%84%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.0.8.19.</span> <span class="toc-text">路径规划器（PP，Path Planner）它是基于 Dijkstra 的生成路径的方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E5%AE%9A%E9%87%8F%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.0.9.</span> <span class="toc-text">B.定量实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-%E5%AE%9A%E6%80%A7%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.0.10.</span> <span class="toc-text">C.定性实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.0.11.</span> <span class="toc-text">结论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88DRL%EF%BC%89%E7%9A%84%E7%9B%AE%E6%A0%87%E9%A9%B1%E5%8A%A8%E5%9E%8B%E5%85%A8%E8%87%AA%E4%B8%BB%E6%8E%A2%E7%B4%A2%E7%B3%BB%E7%BB%9F%EF%BC%88GDAE%EF%BC%89"><span class="toc-number">1.0.11.1.</span> <span class="toc-text">基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A0%E9%9C%80%E4%BA%BA%E5%B7%A5%E5%B9%B2%E9%A2%84%EF%BC%8C%E5%8D%B3%E5%8F%AF%E5%AF%BC%E8%88%AA%E8%87%B3%E6%8C%87%E5%AE%9A%E7%9B%AE%E6%A0%87%E5%B9%B6%E8%AE%B0%E5%BD%95%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF%EF%BC%88%E5%AF%BC%E8%88%AA%E5%8E%BB%E7%9B%AE%E6%A0%87%E7%82%B9%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%BB%BA%E5%9B%BE%EF%BC%89"><span class="toc-number">1.0.11.2.</span> <span class="toc-text">无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%9C%89%E6%95%88%E7%BB%93%E5%90%88%E4%BA%86%E5%8F%8D%E5%BA%94%E5%BC%8F%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%AF%BC%E8%88%AA%E7%AD%96%E7%95%A5%E5%92%8C%E5%85%A8%E5%B1%80%E5%AF%BC%E8%88%AA%E7%AD%96%E7%95%A5"><span class="toc-number">1.0.11.3.</span> <span class="toc-text">系统有效结合了反应式的本地导航策略和全局导航策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97%E5%BC%95%E5%85%A5%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%B3%BB%E7%BB%9F%EF%BC%9A%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%97%A0%E9%9C%80%E7%94%9F%E6%88%90%E6%98%BE%E5%BC%8F%E8%B7%AF%E5%BE%84%E5%8D%B3%E5%8F%AF%E7%A7%BB%E5%8A%A8%EF%BC%8C%E8%80%8C%E8%BF%99%E4%B8%80%E6%9C%BA%E5%88%B6%E7%9A%84%E4%B8%8D%E8%B6%B3%E5%88%99%E9%80%9A%E8%BF%87%E5%BC%95%E5%85%A5%E5%85%A8%E5%B1%80%E5%AF%BC%E8%88%AA%E7%AD%96%E7%95%A5%E5%BE%97%E5%88%B0%E4%BA%86%E5%BC%A5%E8%A1%A5"><span class="toc-number">1.0.11.4.</span> <span class="toc-text">将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AF%BC%E8%88%AA%E6%80%A7%E8%83%BD%E6%8E%A5%E8%BF%91%E4%BA%8E%E5%9F%BA%E4%BA%8E%E5%B7%B2%E7%9F%A5%E5%9C%B0%E5%9B%BE%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%E5%99%A8%E6%89%80%E5%BE%97%E7%9A%84%E6%9C%80%E4%BC%98%E8%A7%A3"><span class="toc-number">1.0.11.5.</span> <span class="toc-text">系统的导航性能接近于基于已知地图路径规划器所得的最优解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GDAE-%E7%B3%BB%E7%BB%9F%E4%BE%9D%E8%B5%96%E7%9B%B4%E6%8E%A5%E7%9A%84%E4%BC%A0%E6%84%9F%E5%99%A8%E8%BE%93%E5%85%A5%E8%80%8C%E9%9D%9E%E4%BB%8E%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%9C%B0%E5%9B%BE%E7%94%9F%E6%88%90%E8%B7%AF%E5%BE%84%EF%BC%8C%E5%9B%A0%E6%AD%A4%E5%9C%A8%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%96%B9%E9%9D%A2%E8%A1%A8%E7%8E%B0%E6%9B%B4%E4%BD%B3"><span class="toc-number">1.0.11.6.</span> <span class="toc-text">GDAE 系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8B%A5%E5%B8%8C%E6%9C%9B%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%B3%9B%E5%8C%96%E8%87%B3%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E4%BA%BA%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%B0%86%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E5%8A%9B%E5%AD%A6%E4%BD%9C%E4%B8%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%80%E4%B8%AA%E8%BE%93%E5%85%A5%E7%8A%B6%E6%80%81%EF%BC%8C%E5%B9%B6%E6%8D%AE%E6%AD%A4%E5%BC%80%E5%B1%95%E8%AE%AD%E7%BB%83%E3%80%82%E5%8F%AA%E9%9C%80%E6%8F%90%E4%BE%9B%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E5%8A%9B%E5%AD%A6%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%8D%B3%E5%8F%AF%E5%9C%A8%E4%B8%8D%E5%90%8C%E5%B9%B3%E5%8F%B0%E4%B8%8A%E5%AE%9E%E7%8E%B0%E6%9C%80%E4%BC%98%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%AF%BC%E8%88%AA%E3%80%82"><span class="toc-number">1.0.11.7.</span> <span class="toc-text">若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A5%E4%B8%8B%E6%9D%A5%E7%9A%84%E7%A0%94%E7%A9%B6%EF%BC%9A"><span class="toc-number">1.0.11.8.</span> <span class="toc-text">接下来的研究：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%EF%BC%88LSTM%EF%BC%89%E7%BB%93%E6%9E%84%E4%B9%9F%E5%8F%AF%E8%83%BD%E6%9C%89%E5%8A%A9%E4%BA%8E%E7%BC%93%E8%A7%A3%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98%EF%BC%8C%E5%B9%B6%E8%BE%85%E5%8A%A9%E8%A7%84%E9%81%BF%E5%BD%93%E5%89%8D%E4%BC%A0%E6%84%9F%E5%99%A8%E8%A7%86%E9%87%8E%E4%B9%8B%E5%A4%96%E7%9A%84%E9%9A%9C%E7%A2%8D%E7%89%A9"><span class="toc-number">1.0.11.9.</span> <span class="toc-text">引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86"><span class="toc-number">1.0.12.</span> <span class="toc-text">代码部分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E9%87%8F%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">1.0.12.1.</span> <span class="toc-text">两个量的定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Episode%EF%BC%9A"><span class="toc-number">1.0.12.2.</span> <span class="toc-text">Episode：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%83%E6%98%AF%E5%90%8E%E7%BB%AD%E6%AD%A5%E9%AA%A4%E7%9A%84%E9%9B%86%E5%90%88%EF%BC%8C%E7%9B%B4%E5%88%B0%E8%BE%BE%E5%88%B0%E5%85%B6%E4%B8%AD%E4%B8%80%E4%B8%AA%E7%BB%88%E6%AD%A2%E6%9D%A1%E4%BB%B6%EF%BC%88%E7%BB%88%E6%AD%A2%E6%9D%A1%E4%BB%B6%EF%BC%9A%E8%BE%BE%E5%88%B0%E7%9B%AE%E6%A0%87%E3%80%81%E4%B8%8E%E9%9A%9C%E7%A2%8D%E7%89%A9%E7%A2%B0%E6%92%9E%E6%88%96%E8%BE%BE%E5%88%B0%E6%9C%80%E5%A4%A7%E6%AD%A5%E6%95%B0max-ep%EF%BC%89"><span class="toc-number">1.0.12.3.</span> <span class="toc-text">它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max_ep）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%83%E5%8F%AF%E4%BB%A5%E7%90%86%E8%A7%A3%E6%98%AF%E4%B8%80%E4%B8%AA%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%8C%E5%A6%82%E6%9E%9C%E8%BE%BE%E5%88%B0%E7%BB%88%E6%AD%A2%E6%9D%A1%E4%BB%B6%E5%B0%B1%E5%BC%80%E5%A7%8B%E6%96%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%8C%E5%B9%B6%E4%B8%94%E4%BC%B4%E9%9A%8F%E7%9D%80%E7%8E%AF%E5%A2%83%E7%9A%84%E6%94%B9%E5%8F%98%EF%BC%8C%E5%8C%85%E6%8B%AC%E5%8C%BA%E5%9F%9F%E5%86%85%E7%9A%84%E5%A2%99%E4%BD%93%E4%BD%8D%E7%BD%AE%E6%94%B9%E5%8F%98%E3%80%81%E9%9A%9C%E7%A2%8D%E7%89%A9%E7%9A%84%E4%BD%8D%E7%BD%AE%E6%94%B9%E5%8F%98%E3%80%81agent%E7%9A%84%E5%88%9D%E5%A7%8B%E4%BD%8D%E7%BD%AE%E6%94%B9%E5%8F%98%E5%92%8C%E7%9B%AE%E6%A0%87%E7%82%B9%E4%BD%8D%E7%BD%AE%E7%9A%84%E6%94%B9%E5%8F%98"><span class="toc-number">1.0.12.4.</span> <span class="toc-text">它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Epoch%EF%BC%9A"><span class="toc-number">1.0.12.5.</span> <span class="toc-text">Epoch：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E8%AF%84%E4%BC%B0%E4%B9%8B%E9%97%B4%E7%9A%84%E5%90%8E%E7%BB%AD%E4%BA%8B%E4%BB%B6%E6%95%B0%EF%BC%88episode%EF%BC%89%E6%88%96%E8%80%85%E6%97%B6%E9%97%B4%E6%AD%A5%E9%95%BF%EF%BC%88timesteps%EF%BC%89"><span class="toc-number">1.0.12.6.</span> <span class="toc-text">执行评估之间的后续事件数（episode）或者时间步长（timesteps）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AAepoch%E8%BF%90%E8%A1%8C5000%E4%B8%AA%E6%AD%A5%E9%AA%A4step%EF%BC%88%E5%AF%B9%E5%BA%94%E4%BB%A3%E7%A0%81%E4%B8%AD%E7%9A%84-eval-freq-%E8%BF%99%E4%B8%AA%E5%8F%82%E6%95%B0%EF%BC%89%EF%BC%8C%E6%AD%A5%E9%95%BF%E4%B8%BA0-1%E7%A7%92%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%E4%B8%80%E4%B8%AAepoch%E5%A4%A7%E6%A6%82%E8%A6%81%E8%BF%90%E8%A1%8C8%E5%88%86%E5%A4%9A%E9%92%9F"><span class="toc-number">1.0.12.7.</span> <span class="toc-text">一个epoch运行5000个步骤step（对应代码中的 eval_freq 这个参数），步长为0.1秒，也就是说一个epoch大概要运行8分多钟</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url(http://picbed.yanzu.tech/img/post_cover/p17.png);"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By yanzu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">I wish you to become your own sun, no need to rely on who's light.<p><a target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用JsDelivr为静态资源提供CDN加速"></a> &nbsp;<a target="_blank" href="https://vercel.com/ "><img src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://vercel.com/ "><img src="https://img.shields.io/badge/Hosted-Coding-0cedbe?style=flat&logo=Codio" title="本站采用双线部署，联通线路托管于Coding"></a>&nbsp;<a target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>(() => {
  const panguFn = () => {
    if (typeof pangu === 'object') pangu.autoSpacingPage()
    else {
      btf.getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
        .then(() => {
          pangu.autoSpacingPage()
        })
    }
  }

  const panguInit = () => {
    if (false){
      GLOBAL_CONFIG_SITE.isPost && panguFn()
    } else {
      panguFn()
    }
  }

  btf.addGlobalFn('pjaxComplete', panguInit, 'pangu')
  document.addEventListener('DOMContentLoaded', panguInit)
})()</script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.isShuoshuo
  const option = {"lang":"zh-CN"}

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo-api-eta-one.vercel.app',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://twikoo-api-eta-one.vercel.app',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script async src="/js/fps.js"></script><script async src="/js/title.js"></script><div class="aplayer no-destroy" data-id="13400648726" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true" data-lrcType="-1"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script>(() => {
  const destroyAplayer = () => {
    if (window.aplayers) {
      for (let i = 0; i < window.aplayers.length; i++) {
        if (!window.aplayers[i].options.fixed) {
          window.aplayers[i].destroy()
        }
      }
    }
  }

  const runMetingJS = () => {
    typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()
  }

  btf.addGlobalFn('pjaxSend', destroyAplayer, 'destroyAplayer')
  btf.addGlobalFn('pjaxComplete', loadMeting, 'runMetingJS')
})()</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      pjax.loadUrl('/404.html')
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>